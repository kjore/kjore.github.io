<!DOCTYPE html><html lang="zh-CN" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>《Pass@k 训练》论文深度研读 | kjore's blog</title><meta name="author" content="kjore"><meta name="copyright" content="kjore"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="原文链接：http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2508.10751 《Pass@k 训练》论文深度研读 这篇论文《Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models》，是大型语言模型（LLM）推理能力训练领域的一篇重要技术报告。它不仅提出了一种新">
<meta property="og:type" content="article">
<meta property="og:title" content="《Pass@k 训练》论文深度研读">
<meta property="og:url" content="https://kjore.github.io/2025/11/14/%E7%94%A8%E4%BA%8E%E8%87%AA%E9%80%82%E5%BA%94%E5%B9%B3%E8%A1%A1%E5%A4%A7%E5%9E%8B%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%A9%E7%94%A8%E7%9A%84%20Pass@k%20%E8%AE%AD%E7%BB%83/index.html">
<meta property="og:site_name" content="kjore&#39;s blog">
<meta property="og:description" content="原文链接：http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2508.10751 《Pass@k 训练》论文深度研读 这篇论文《Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models》，是大型语言模型（LLM）推理能力训练领域的一篇重要技术报告。它不仅提出了一种新">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://kjore.github.io/img/head.png">
<meta property="article:published_time" content="2025-11-14T07:28:02.000Z">
<meta property="article:modified_time" content="2025-11-14T15:03:18.211Z">
<meta property="article:author" content="kjore">
<meta property="article:tag" content="论文研读">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://kjore.github.io/img/head.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "《Pass@k 训练》论文深度研读",
  "url": "https://kjore.github.io/2025/11/14/%E7%94%A8%E4%BA%8E%E8%87%AA%E9%80%82%E5%BA%94%E5%B9%B3%E8%A1%A1%E5%A4%A7%E5%9E%8B%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%A9%E7%94%A8%E7%9A%84%20Pass@k%20%E8%AE%AD%E7%BB%83/",
  "image": "https://kjore.github.io/img/head.png",
  "datePublished": "2025-11-14T07:28:02.000Z",
  "dateModified": "2025-11-14T15:03:18.211Z",
  "author": [
    {
      "@type": "Person",
      "name": "kjore",
      "url": "https://kjore.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://kjore.github.io/2025/11/14/%E7%94%A8%E4%BA%8E%E8%87%AA%E9%80%82%E5%BA%94%E5%B9%B3%E8%A1%A1%E5%A4%A7%E5%9E%8B%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%A9%E7%94%A8%E7%9A%84%20Pass@k%20%E8%AE%AD%E7%BB%83/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '《Pass@k 训练》论文深度研读',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 8.0.0"></head><body><div id="web_bg" style="background-image: url(/img/bg.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/head.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-th-large"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/bg.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/head.png" alt="Logo"><span class="site-name">kjore's blog</span></a><a class="nav-page-title" href="/"><span class="site-name">《Pass@k 训练》论文深度研读</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-th-large"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">《Pass@k 训练》论文深度研读</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-11-14T07:28:02.000Z" title="发表于 2025-11-14 15:28:02">2025-11-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-11-14T15:03:18.211Z" title="更新于 2025-11-14 23:03:18">2025-11-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>原文链接：<a
target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10751">http://arxiv.org/abs/2508.10751</a></p>
<h1 id="passk-训练论文深度研读">《Pass@k 训练》论文深度研读</h1>
<p>这篇论文《Pass@k Training for Adaptively Balancing Exploration and
Exploitation of Large Reasoning
Models》，是大型语言模型（LLM）推理能力训练领域的一篇重要技术报告。它不仅提出了一种新颖且效果显著的训练方法，更重要的是，它为解决强化学习（ReinCforcement
Learning,
RL）中的一个经典难题——“探索与利用的平衡”——提供了深刻的洞见。</p>
<p>我们的目标是，在阅读完本报告后，您不仅能理解这篇论文“做了什么”，更能深刻理解它“为什么这么做”以及“它背后的数学原理”。</p>
<hr />
<h2
id="第一部分问题的提出摘要与第1节引言">第一部分：问题的提出（摘要与第1节：引言）</h2>
<p>在进入复杂的公式之前，我们必须首先理解论文试图解决的“问题”是什么。</p>
<h3 id="核心场景rlvr-与大型推理模型">1.1 核心场景：RLVR
与大型推理模型</h3>
<p>论文的背景是使用<strong>强化学习与可验证奖励（Reinforcement Learning
with Verifiable Rewards, RLVR）</strong>来训练大型推理模型（Large
Reasoning Models, LRMs）。</p>
<ul>
<li><strong>RLVR 是什么？</strong> 这是一种训练方法。它让 LLM
像一个“学生”一样去“探索”——尝试生成各种答案或推理过程。然后，一个“验证器”（Verifier，可以是一个程序或另一个模型）会像“老师”一样，对这些答案进行“验证”并给出“奖励”（Reward）——例如，答案正确得
1 分，错误得 0 分。</li>
<li><strong>目标：</strong> LLM
通过不断地试错，学习如何最大化自己能获得的奖励，从而“极大地提升其推理能力”。DeepSeek
R1 和 OpenAI 的一些模型都受益于此类训练。</li>
</ul>
<h3 id="传统方法的困境pass1-训练与局部最优陷阱">1.2
传统方法的困境：Pass@1 训练与“局部最优”陷阱</h3>
<p>目前，标准的 RLVR 训练方法被称为 <strong>Pass@1
训练</strong>。顾名思义，它的优化目标是让模型在“第 1
次尝试时就生成最自信的正确答案”。</p>
<p>然而，论文的作者指出，这种方法存在一个致命缺陷：<strong>它导致了“探索（Exploration）”与“利用（Exploitation）”的严重失衡</strong>
。</p>
<ul>
<li><strong>探索 vs. 利用：</strong>
“利用”是指模型重复那些它已知能获得高奖励的“保守行为”（例如，一个它已经背过的、虽然有瑕疵但能碰巧答对的推理路径）。“探索”则是指模型去尝试“新颖且多样的行为”（例如，一个全新的、它不确定是否能成功的推理路径）。</li>
<li><strong>Pass@1 的惩罚机制：</strong>
问题在于，推理是一个复杂的多步骤过程。在 Pass@1 训练中：
<ol type="1">
<li>一个“<strong>包含正确思路但最终答案错误</strong>”的响应（例如，9
步推理都对，最后 1 步计算错了），会收到 0
分的惩罚。模型因此学习到要“避免”这条（大部分）正确的路径，从而<strong>惩罚了有价值的探索</strong>
。</li>
<li>一个“<strong>包含错误逻辑但碰巧答案正确</strong>”的响应（例如，逻辑错误但“侥幸”蒙对了数字），反而会收到
1 分的奖励。模型因此学会了“利用”这条错误的路径。</li>
</ol></li>
<li><strong>“局部最优”陷阱：</strong> 这种“次优的奖励信号”
导致模型变得“保守”。它很快会找到一两条“凑合能用”的路径（即“局部最优”），然后就“困在”那里，停止了对“全局最优”（真正正确的推理能力）的探索。</li>
</ul>
<h3 id="论文的核心方案passk-训练">1.3 论文的核心方案：Pass@k 训练</h3>
<p>为了打破这个“陷阱”，作者提出了 <strong>Pass@k 训练</strong>。</p>
<ul>
<li><strong>什么是 Pass@k？</strong> Pass@k
是一个评估指标，用于衡量模型在 <span
class="math inline"><em>k</em></span>
次尝试内，能否产生<strong>至少一个</strong>正确的答案。</li>
<li><strong>Pass@k 作为训练奖励：</strong>
论文的核心创新就是将这个“评估指标”用作“训练奖励”。</li>
<li><strong>为什么 Pass@k 有效？</strong>
<ol type="1">
<li><strong>更高的容错性：</strong> Pass@k
对“不正确的响应”有更高的容（）。在 Pass@k 训练中，一个包含 9
步正确推理但最后 1 步错误的响应，只要在 <span
class="math inline"><em>k</em></span>
次尝试中的另一次是完全正确的，那么这一整组（包括那个不完美的探索）都会获得正向的激励。这<strong>保护了模型的探索行为</strong>。</li>
<li><strong>激励多样性：</strong> 为了最大化 Pass@k
奖励，一个“聪明”的策略（Policy） 必须学会生成 <span
class="math inline"><em>k</em></span>
个<strong>彼此不同</strong>、<strong>覆盖不同解空间区域</strong>的候选方案，而不是
<span class="math inline"><em>k</em></span>
个高度相似的方案。这在机制上<strong>强制模型进行探索和多样化</strong>。</li>
</ol></li>
</ul>
<h3 id="核心假说探索与利用的相互增强">1.4
核心假说：探索与利用的“相互增强”</h3>
<p>这篇论文的中心论点（或假说）是：<strong>探索和利用并非天生冲突，它们可以相互增强</strong>
。</p>
<p>传统的 Pass@1 训练让它们陷入了冲突。而 Pass@k
训练通过增强“探索”（让模型敢于尝试并发现更广阔的解空间），反过来为“利用”（在后续训练中打磨出最佳答案）提供了更高质量的基础。模型因此得以逃离“局部最优”，持续进步。</p>
<hr />
<h2 id="第二部分技术框架将-passk-实现为奖励第2节">第二部分：技术框架：将
Pass@k 实现为奖励（第2节）</h2>
<p>这是论文的数学核心，我们将按照您的要求，逐个拆解本节中的所有公式。</p>
<h3 id="基线回顾pass1-训练的数学表述第2.1节">2.1 基线回顾：Pass@1
训练的数学表述（第2.1节）</h3>
<p>首先，论文定义了基线（即 Pass@1 训练）的数学符号 。</p>
<ul>
<li>一个问题（prompt）记为 <span
class="math inline"><em>x</em></span>。</li>
<li>模型的参数为 <span
class="math inline"><em>θ</em></span>，策略（即模型本身）记为 <span
class="math inline"><em>π</em><sub><em>θ</em></sub></span>。</li>
<li>模型生成的响应（一个词元序列）记为 <span
class="math inline"><em>ŷ</em> = {<em>t</em><sub>1</sub>, <em>t</em><sub>2</sub>, …, <em>t</em><sub><em>l</em></sub>}</span>。</li>
<li>验证器提供一个奖励 <span
class="math inline"><em>R</em>(<em>y</em>, <em>ŷ</em>)</span>，正确为
<span
class="math inline"><em>R</em><sub><em>p</em><em>o</em><em>s</em></sub> = 1</span>，错误为
<span
class="math inline"><em>R</em><sub><em>n</em><em>e</em><em>g</em></sub> = 0</span>。</li>
<li><span
class="math inline"><em>N</em><sub><em>r</em><em>o</em><em>l</em><em>l</em><em>o</em><em>u</em><em>t</em></sub></span>
指的是模型为<strong>同一个问题 <span
class="math inline"><em>x</em></span></strong>
生成的响应数量（即采样次数）。</li>
</ul>
<p><strong>公式 (1) <span class="math inline">$\overline{R}$</span>
(平均奖励) 与 公式 (2) <span class="math inline"><em>σ</em></span>
(奖励标准差)</strong></p>
<p><span class="math display">$$
\overline{R}=\frac{1}{N_{rollout}}\sum_{i=1}^{N_{rollout}}R_{i}
$$</span></p>
<p><span class="math display">$$
\sigma=\frac{1}{N_{rollout}}\sqrt{\sum_{i=1}^{N_{rollout}}(R_{i}-\overline{R})^{2}}
$$</span></p>
<ul>
<li><strong>含义：</strong> 这两个公式计算的是在 <span
class="math inline"><em>N</em><sub><em>r</em><em>o</em><em>l</em><em>l</em><em>o</em><em>u</em><em>t</em></sub></span>
次响应中，模型获得奖励 <span
class="math inline"><em>R</em><sub><em>i</em></sub></span>
的<strong>平均值</strong>（<span
class="math inline">$\overline{R}$</span>）和<strong>标准差</strong>（<span
class="math inline"><em>σ</em></span>）。</li>
<li><strong>作用：</strong>
它们用于<strong>奖励的标准化（Normalization）</strong>。在强化学习中，奖励的绝对值（比如
1 分）意义不大，重要的是这个 1
分“<strong>比平均水平好多少</strong>”。<span
class="math inline">$\overline{R}$</span>
提供了这个“平均水平”的基线（Baseline）。</li>
</ul>
<p><strong>公式 (3) <span
class="math inline"><em>Â</em><sub><em>i</em>, <em>t</em></sub></span>
(优势函数)</strong></p>
<p><span class="math display">$$
\hat{A}_{i,1}=\hat{A}_{i,2}=\cdot\cdot\cdot=\hat{A}_{i,|\hat{y}_{i}|}=\frac{R_{i}-R}{\sigma}
$$</span></p>
<ul>
<li><strong>含义：</strong> <span class="math inline"><em>Â</em></span>
代表<strong>优势（Advantage）</strong>。这个公式计算了第 <span
class="math inline"><em>i</em></span> 个响应 <span
class="math inline"><em>ŷ</em><sub><em>i</em></sub></span>
的“标准化奖励”，在统计学上称为“z-score”。</li>
<li><strong>拆解：</strong>
<ul>
<li><span class="math inline">$R_i - \overline{R}$</span>：计算第 <span
class="math inline"><em>i</em></span>
个响应的奖励与平均奖励的差距。</li>
<li><span class="math inline">$\frac{\dots}{\sigma}$</span>：用标准差
<span class="math inline"><em>σ</em></span> 对这个差距进行缩放。</li>
</ul></li>
<li><strong>作用：</strong> <span class="math inline"><em>Â</em></span>
是驱动模型更新的核心信号：
<ul>
<li>如果 <span class="math inline"><em>Â</em> &gt; 0</span>（即 <span
class="math inline">$R_i &gt; \overline{R}$</span>），说明这个响应 <span
class="math inline"><em>ŷ</em><sub><em>i</em></sub></span>
<strong>好于平均水平</strong>，模型将被更新，以<strong>提高</strong>未来生成这个响应中所有词元
<span class="math inline"><em>t</em></span> 的概率。</li>
<li>如果 <span class="math inline"><em>Â</em> &lt; 0</span>（即 <span
class="math inline">$R_i &lt; \overline{R}$</span>），说明这个响应 <span
class="math inline"><em>ŷ</em><sub><em>i</em></sub></span>
<strong>差于平均水平</strong>，模型将被更新，以<strong>降低</strong>未来生成这些词元的概率。</li>
</ul></li>
<li><strong>关键局限（信用分配问题）：</strong> 注意，<span
class="math inline"><em>Â</em><sub><em>i</em>, 1</sub> = <em>Â</em><sub><em>i</em>, 2</sub> = …</span>
这意味着响应 <span
class="math inline"><em>ŷ</em><sub><em>i</em></sub></span>
中的<strong>每一个词元（token）</strong> <span
class="math inline"><em>t</em></span>，都<strong>共享完全相同</strong>的优势值
<span class="math inline"><em>Â</em></span>。这就是第 1
节中讨论的“信用分配问题”的数学体现。如果 <span
class="math inline"><em>R</em><sub><em>i</em></sub> = 0</span>（最终答案错误），那么
<span class="math inline"><em>Â</em></span>
就是负数，导致<strong>所有</strong>词元（包括那些正确的推理步骤）都受到惩罚。</li>
</ul>
<p><strong>公式 (4) <span class="math inline">𝒥(<em>θ</em>)</span>
(目标函数)</strong></p>
<p><span
class="math display">𝒥(<em>θ</em>) = 𝔼<sub>(<em>q</em>, <em>a</em>) ∼ <em>D</em>, [<em>ŷ</em><sub><em>i</em></sub>]<sub><em>i</em> = 1</sub><sup><em>G</em></sup> ∼ <em>π</em><sub><em>θ</em></sub>(⋅|<em>q</em>)</sub>[…]</span></p>
<p><em>（注：原文公式较长且复杂，我们拆解其核心部分）</em></p>
<ul>
<li><strong>含义：</strong> <span
class="math inline">𝒥(<em>θ</em>)</span>
是模型优化的最终<strong>目标函数（Objective
Function）</strong>，在机器学习中也常被称为“损失函数”（Loss
Function）。模型的目标就是最大化 <span
class="math inline">𝒥(<em>θ</em>)</span>。</li>
<li><strong>核心思想（策略梯度 Policy Gradient）：</strong> 其核心是
<span
class="math inline"><em>r</em><sub><em>i</em>, <em>t</em></sub><em>Â</em><sub><em>i</em>, <em>t</em></sub></span>
这一项（及其变种）。
<ul>
<li><span
class="math inline"><em>Â</em><sub><em>i</em>, <em>t</em></sub></span>
是我们刚讨论过的“优势”（即奖励信号）。</li>
<li><span
class="math inline"><em>r</em><sub><em>i</em>, <em>t</em></sub></span>
是一个概率比率，衡量模型“现在”有多大可能采取动作 <span
class="math inline"><em>t</em></span>（相较于“过去”采样时）。</li>
<li>整个优化的目标是：如果 <span
class="math inline"><em>Â</em><sub><em>i</em>, <em>t</em></sub></span>
是正的，就调整 <span class="math inline"><em>θ</em></span> 来增大 <span
class="math inline"><em>r</em><sub><em>i</em>, <em>t</em></sub></span>（让这个好动作更容易发生）；如果
<span
class="math inline"><em>Â</em><sub><em>i</em>, <em>t</em></sub></span>
是负的，就减小 <span
class="math inline"><em>r</em><sub><em>i</em>, <em>t</em></sub></span>（让这个坏动作更难发生）。</li>
</ul></li>
<li><strong>Clip 和 <span
class="math inline"><em>D</em><sub><em>K</em><em>L</em></sub></span>：</strong>
公式中的 <span
class="math inline"><em>m</em><em>i</em><em>n</em>(…, <em>c</em><em>l</em><em>i</em><em>p</em>(…))</span>
和 <span
class="math inline"><em>β</em><em>D</em><sub><em>K</em><em>L</em></sub></span>
是来自 PPO (Proximal Policy Optimization)
算法的“稳定化”措施。它们是“安全护栏”，防止模型在单次更新中“步子迈得太大”而导致训练崩溃
。</li>
</ul>
<h3 id="朴素实现passk-训练之全采样第2.2节">2.2 朴素实现：Pass@k
训练之“全采样”（第2.2节）</h3>
<p>现在，我们进入论文的核心方案 Pass@k 训练。</p>
<p><strong>公式 (5) <span
class="math inline"><em>P</em><em>a</em><em>s</em><em>s</em>@<em>k</em></span>
(Pass@k 指标的定义)</strong></p>
<p><span
class="math display"><em>P</em><em>a</em><em>s</em><em>s</em>@<em>k</em> = 𝔼<sub>(<em>x</em>, <em>y</em>) ∼ <em>D</em>, (<em>ŷ</em><sub><em>i</em></sub>)<sub><em>i</em> = 1</sub><sup><em>k</em></sup> ∼ <em>π</em><sub><em>θ</em></sub>(⋅|<em>x</em>)</sub></span></p>
<ul>
<li><strong>含义：</strong> 这是 Pass@k 指标的严格数学定义 。</li>
<li><strong>拆解：</strong>
<ul>
<li><span
class="math inline">𝔼[…]</span>：表示“期望值”（即“平均而言”）。</li>
<li><span
class="math inline">(<em>ŷ</em><sub><em>i</em></sub>)<sub><em>i</em> = 1</sub><sup><em>k</em></sup> ∼ <em>π</em><sub><em>θ</em></sub>(⋅|<em>x</em>)</span>：表示从策略
<span class="math inline"><em>π</em><sub><em>θ</em></sub></span> 中采样
<span class="math inline"><em>k</em></span> 个响应。</li>
<li><span
class="math inline"><em>m</em><em>a</em><em>x</em>(<em>R</em><sub>1</sub>, ..., <em>R</em><sub><em>k</em></sub>)</span>：计算这
<span class="math inline"><em>k</em></span>
个响应奖励中的<strong>最大值</strong>。</li>
</ul></li>
<li><strong>为什么这个公式等同于 Pass@k？</strong> 因为奖励 <span
class="math inline"><em>R</em><sub><em>i</em></sub></span> 只有
0（错误）和 1（正确）两种。
<ul>
<li>如果 <span class="math inline"><em>k</em></span>
个响应<strong>全是</strong> 0，那么 <span
class="math inline"><em>m</em><em>a</em><em>x</em>(…)</span> 的结果是
0。</li>
<li>只要 <span class="math inline"><em>k</em></span>
个响应中<strong>至少有 1 个</strong>是 1，那么 <span
class="math inline"><em>m</em><em>a</em><em>x</em>(…)</span> 的结果就是
1。</li>
<li>因此，这个期望值 <span class="math inline">𝔼[…]</span>
计算的正是“<span class="math inline"><em>k</em></span> 次尝试中至少有 1
次成功的平均概率”。</li>
</ul></li>
</ul>
<p><strong>“全采样”（Full Sampling）的实现</strong></p>
<p>这是 Pass@k 训练最基础（朴素）的实现方式 ：</p>
<ol type="1">
<li><strong>生成：</strong> 为一个问题 <span
class="math inline"><em>x</em></span>，生成 <span
class="math inline"><em>N</em><sub><em>r</em><em>o</em><em>l</em><em>l</em><em>o</em><em>u</em><em>t</em></sub></span>
个响应。</li>
<li><strong>分组：</strong> 将这 <span
class="math inline"><em>N</em><sub><em>r</em><em>o</em><em>l</em><em>l</em><em>o</em><em>u</em><em>t</em></sub></span>
个响应<strong>不重叠地</strong>（disjointly）划分为 <span
class="math inline"><em>N</em><sup><em>g</em><em>r</em><em>o</em><em>u</em><em>p</em></sup> = ⌊<em>N</em><sub><em>r</em><em>o</em><em>l</em><em>l</em><em>o</em><em>u</em><em>t</em></sub>/<em>k</em>⌋</span>
个组，每组 <span class="math inline"><em>k</em></span>
个响应。多余的响应被丢弃。</li>
<li><strong>计算“组奖励”：</strong> 对于第 <span
class="math inline"><em>j</em></span> 个组 <span
class="math inline"><em>ŷ</em><sup><em>j</em></sup> = {<em>ŷ</em><sub>1</sub><sup><em>j</em></sup>, …, <em>ŷ</em><sub><em>k</em></sub><sup><em>j</em></sup>}</span>，计算一个<strong>组奖励</strong>
<span
class="math inline"><em>R</em><sup><em>j</em></sup> = <em>m</em><em>a</em><em>x</em>(<em>R</em><sub>1</sub><sup><em>j</em></sup>, …, <em>R</em><sub><em>k</em></sub><sup><em>j</em></sup>)</span>。</li>
<li><strong>计算“组优势”：</strong> 使用这些“组奖励” <span
class="math inline"><em>R</em><sup><em>j</em></sup></span>（而不是单个响应的
<span
class="math inline"><em>R</em><sub><em>i</em></sub></span>），代入公式
(1), (2), (3) 中，计算出每个组的<strong>组优势</strong> <span
class="math inline"><em>Â</em><sup><em>j</em></sup></span>。</li>
<li><strong>分配优势：</strong> 将这个 <span
class="math inline"><em>Â</em><sup><em>j</em></sup></span>
<strong>同时分配给组内的所有 <span class="math inline"><em>k</em></span>
个响应</strong>。</li>
</ol>
<p><strong>图 3 的实证（Figure 3）</strong></p>
<p>这个朴素的实现已经带来了显著效果。如图 3 所示 ：</p>
<ul>
<li><strong>Pass@1 训练（虚线）：</strong> Pass@k
性能（绿色虚线）很快就“停滞”了，表明模型陷入了局部最优，<strong>忘记了如何探索</strong>。</li>
<li><strong>Pass@k 训练（实线）：</strong> Pass@k
性能（蓝色实线）则获得了“持续的改进”，表明模型成功逃离了局部最优，其探索能力得到了提升。</li>
</ul>
<h3 id="高效实现passk-训练之bootstrap-采样第2.3节">2.3 高效实现：Pass@k
训练之“Bootstrap 采样”（第2.3节）</h3>
<p>“全采样”很浪费算力（丢弃了响应），而且分组太少。为此，论文提出了第一个改进版：“Bootstrap
采样”（Bootstrap Sampling）。</p>
<ul>
<li><strong>工作流程：</strong>
<ol type="1">
<li><strong>生成：</strong> 同样生成 <span
class="math inline"><em>N</em><sub><em>r</em><em>o</em><em>l</em><em>l</em><em>o</em><em>u</em><em>t</em></sub></span>
个响应，作为一个“响应池”（pool）。</li>
<li><strong>采样组：</strong> 为了构建第 <span
class="math inline"><em>j</em></span>
个组，从“响应池”中<strong>随机</strong>（Bootstrap）采样 <span
class="math inline"><em>k</em></span>
个<strong>不同</strong>的响应。</li>
<li><strong>重复：</strong> 重复这个过程 <span
class="math inline"><em>N</em><sup><em>g</em><em>r</em><em>o</em><em>u</em><em>p</em></sup></span>
次（例如，设置 <span
class="math inline"><em>N</em><sup><em>g</em><em>r</em><em>o</em><em>u</em><em>p</em></sup> = <em>N</em><sub><em>r</em><em>o</em><em>l</em><em>l</em><em>o</em><em>u</em><em>t</em></sub></span>）。</li>
</ol></li>
<li><strong>核心区别：</strong>
在“全采样”中，一个响应只属于一个组。而在“Bootstrap 采样”中，一个响应
<span class="math inline"><em>ŷ</em><sub><em>i</em></sub></span>
<strong>可以同时属于多个随机组</strong>。这更充分地利用了生成的 <span
class="math inline"><em>N</em><sub><em>r</em><em>o</em><em>l</em><em>l</em><em>o</em><em>u</em><em>t</em></sub></span>
个样本。</li>
</ul>
<p><strong>公式 (6) <span
class="math inline"><em>Â</em><sub><em>i</em></sub></span> (Bootstrap
下的响应优势)</strong></p>
<p><span class="math display">$$
\hat{A}_{i}=\sum_{j=1}^{N^{group}}\hat{A}^{j}\cdot\mathbb{I}[\hat{y}_{i}\in\hat{y}^{j}]
$$</span></p>
<ul>
<li><strong>含义：</strong> 这个公式计算响应 <span
class="math inline"><em>ŷ</em><sub><em>i</em></sub></span> 的最终优势值
。</li>
<li><strong>拆解：</strong>
<ul>
<li><span class="math inline"><em>Â</em><sup><em>j</em></sup></span>
是第 <span class="math inline"><em>j</em></span>
个随机组的“组优势”（同样通过组奖励 <span
class="math inline"><em>R</em><sup><em>j</em></sup> = <em>m</em><em>a</em><em>x</em>(…)</span>
计算）。</li>
<li><span
class="math inline">𝕀[<em>ŷ</em><sub><em>i</em></sub> ∈ <em>ŷ</em><sup><em>j</em></sup>]</span>
是一个“<strong>指示函数</strong>”（Indicator Function）。如果 <span
class="math inline"><em>ŷ</em><sub><em>i</em></sub></span>
<strong>在</strong>第 <span class="math inline"><em>j</em></span>
组中，它就等于 1；如果<strong>不在</strong>，它就等于 0。</li>
<li><span class="math inline">$\sum_{j=1}^{N^{group}}$</span> 表示对所有
<span
class="math inline"><em>N</em><sup><em>g</em><em>r</em><em>o</em><em>u</em><em>p</em></sup></span>
个组求和。</li>
</ul></li>
<li><strong>作用：</strong> <span
class="math inline"><em>ŷ</em><sub><em>i</em></sub></span>
的总优势，等于它所属的<strong>所有组的优势之和</strong>。</li>
</ul>
<p><strong>图 4 的实证（Figure 4）</strong></p>
<p>Bootstrap 采样的效果立竿见影 ：</p>
<ul>
<li><strong>同等算力下（<span
class="math inline"><em>N</em><sub><em>r</em><em>o</em><em>l</em><em>l</em><em>o</em><em>u</em><em>t</em></sub> = 32</span>）：</strong>
Bootstrap（蓝色）的 Pass@k
性能<strong>远高于</strong>全采样（紫色）。这是因为它构建了更多的组（<span
class="math inline"><em>N</em><sup><em>g</em><em>r</em><em>o</em><em>u</em><em>p</em></sup> = 32</span>
vs <span
class="math inline"><em>N</em><sup><em>g</em><em>r</em><em>o</em><em>u</em><em>p</em></sup> = 32/<em>k</em></span>），对优势的估计更准确（即“方差”更低）。</li>
<li><strong>效率对比：</strong> Bootstrap（<span
class="math inline"><em>N</em><sub><em>r</em><em>o</em><em>l</em><em>l</em><em>o</em><em>u</em><em>t</em></sub> = 32</span>，蓝色）<strong>仅用
1/4 的算力</strong>，就达到了与全采样（<span
class="math inline"><em>N</em><sub><em>r</em><em>o</em><em>l</em><em>l</em><em>o</em><em>u</em><em>t</em></sub> = 128</span>，红色）<strong>几乎相当</strong>的性能。这证明了其<strong>高效率</strong>。</li>
</ul>
<h3 id="终极实现passk-训练之解析推导第2.4节与附录b">2.4 终极实现：Pass@k
训练之“解析推导”（第2.4节与附录B）</h3>
<p>Bootstrap
采样虽然高效，但它仍然是一种“采样”方法，采样天然会引入随机性，即<strong>方差（Variance）</strong>。这会导致训练过程中的“抖动”（如图
5 所示，Bootstrap 采样在 400 步时性能出现了波动）。</p>
<p>第 2.4
节是本文技术上最核心的飞跃：作者提出，<strong>我们根本不需要进行“组采样”！</strong>
我们可以通过数学推导，<strong>直接计算出每个响应的“精确的期望优势”</strong>。</p>
<p>这就是<strong>“解析推导”（Analytical Derivation）</strong>。</p>
<h4
id="步骤一统计所有可能的组公式-7-10">步骤一：统计所有可能的“组”（公式
7-10）</h4>
<p>我们从一个已知的“响应池”出发：总共有 <span
class="math inline"><em>N</em><sub><em>r</em><em>o</em><em>l</em><em>l</em><em>o</em><em>u</em><em>t</em></sub></span>
个响应，其中 <span
class="math inline"><em>N</em><sub><em>p</em><em>o</em><em>s</em></sub></span>
个是正确的（奖励 1），<span
class="math inline"><em>N</em><sub><em>n</em><em>e</em><em>g</em></sub></span>
个是错误的（奖励 0），且 <span
class="math inline"><em>N</em><sub><em>p</em><em>o</em><em>s</em></sub> + <em>N</em><sub><em>n</em><em>e</em><em>g</em></sub> = <em>N</em><sub><em>r</em><em>o</em><em>l</em><em>l</em><em>o</em><em>u</em><em>t</em></sub></span>。</p>
<p>我们现在从理论上考虑，从这 <span
class="math inline"><em>N</em><sub><em>r</em><em>o</em><em>l</em><em>l</em><em>o</em><em>u</em><em>t</em></sub></span>
个响应中，随机抽取 <span class="math inline"><em>k</em></span>
个，能组成的所有可能的“组”。</p>
<p><strong>组合数学 <span
class="math inline">$\binom{n}{k}$</span>：</strong> 在开始前，必须定义
<span class="math inline">$\binom{n}{k}$</span> 符号，读作“n 选 k”（n
choose k）。它代表从 <span class="math inline"><em>n</em></span>
个不同事物中，无序地选出 <span class="math inline"><em>k</em></span>
个事物的所有组合方式的总数。</p>
<p><strong>公式 (8) <span
class="math inline"><em>N</em><sub><em>t</em><em>o</em><em>t</em><em>a</em><em>l</em></sub><sup><em>g</em><em>r</em><em>o</em><em>u</em><em>p</em></sup></span>
(总组数)</strong></p>
<p><span class="math display">$$
N_{total}^{group}=\binom{N_{rollout}}{k}
$$</span></p>
<ul>
<li><strong>含义：</strong> 从 <span
class="math inline"><em>N</em><sub><em>r</em><em>o</em><em>l</em><em>l</em><em>o</em><em>u</em><em>t</em></sub></span>
个总响应中，选出 <span class="math inline"><em>k</em></span>
个来组成一个“组”，总共有多少种选法 。</li>
</ul>
<p><strong>公式 (9) <span
class="math inline"><em>N</em><sub><em>n</em><em>e</em><em>g</em></sub><sup><em>g</em><em>r</em><em>o</em><em>u</em><em>p</em></sup></span>
(负面组数)</strong></p>
<p><span class="math display">$$
N_{neg}^{group}=\binom{N_{neg}}{k}
$$</span></p>
<ul>
<li><strong>含义：</strong> 一个“组”是“负面组”（Negative group，奖励为
0）的<strong>充要条件</strong>是：它所包含的 <span
class="math inline"><em>k</em></span> 个成员<strong>全部</strong>来自
<span
class="math inline"><em>N</em><sub><em>n</em><em>e</em><em>g</em></sub></span>
个错误响应。这个公式计算的就是从 <span
class="math inline"><em>N</em><sub><em>n</em><em>e</em><em>g</em></sub></span>
个错误响应中，选出 <span class="math inline"><em>k</em></span>
个的所有选法 。</li>
</ul>
<p><strong>公式 (10) <span
class="math inline"><em>N</em><sub><em>p</em><em>o</em><em>s</em></sub><sup><em>g</em><em>r</em><em>o</em><em>u</em><em>p</em></sup></span>
(正面组数)</strong></p>
<p><span class="math display">$$
N_{pos}^{group}=N_{total}^{group}-N_{neg}^{group}=\binom{N_{rollout}}{k}-\binom{N_{neg}}{k}
$$</span></p>
<ul>
<li><strong>含义：</strong> 一个“组”是“正面组”（Positive group，奖励为
1）的<strong>充要条件</strong>是：它<strong>至少包含 1
个</strong>正确响应。这个数量等于“总组数”减去“全是负面的组数”。</li>
</ul>
<h4
id="步骤二计算组的精确统计数据公式-11-13">步骤二：计算“组”的精确统计数据（公式
11-13）</h4>
<p>现在我们拥有了所有组的理论分布，我们可以计算出这个分布精确的统计数据，而<strong>无需任何采样</strong>。</p>
<p><strong>公式 (11) <span
class="math inline">$\overline{R}^{group}$</span>
(组的平均奖励)</strong></p>
<p><span class="math display">$$
\overline{R}^{group}=1-\frac{\binom{N_{neg}}{k}}{\binom{N_{rollout}}{k}}
$$</span></p>
<ul>
<li><strong>含义：</strong> 这是所有 <span
class="math inline">$\binom{N_{rollout}}{k}$</span>
个理论组的“期望奖励”或“平均奖励”。</li>
<li><strong>推导（参见附录 B，公式 19-21 ）：</strong>
<ol type="1">
<li>总奖励 = (正面组数 <span class="math inline">×</span> 1) + (负面组数
<span class="math inline">×</span> 0) = <span
class="math inline"><em>N</em><sub><em>p</em><em>o</em><em>s</em></sub><sup><em>g</em><em>r</em><em>o</em><em>u</em><em>p</em></sup></span></li>
<li>平均奖励 <span class="math inline">$\overline{R}^{group}$</span> =
总奖励 / 总组数 = <span
class="math inline">$\frac{N_{pos}^{group}}{N_{total}^{group}}$</span></li>
<li>代入公式 (10)：<span class="math inline">$\overline{R}^{group} =
\frac{N_{total}^{group} - N_{neg}^{group}}{N_{total}^{group}} = 1 -
\frac{N_{neg}^{group}}{N_{total}^{group}}$</span></li>
<li>代入公式 (8) 和 (9)，即得公式 (11)。</li>
</ol></li>
</ul>
<p><strong>公式 (12) <span
class="math inline"><em>σ</em><sup><em>g</em><em>r</em><em>o</em><em>u</em><em>p</em></sup></span>
(组的标准差)</strong></p>
<p><span class="math display">$$
\sigma^{group}=\sqrt{\overline{R}^{group}\times(1-\overline{R}^{group})}
$$</span></p>
<ul>
<li><strong>含义：</strong> 这是所有理论组的“奖励标准差”。</li>
<li><strong>推导（参见附录 B，公式 22-27 ）：</strong>
<ul>
<li>这是一个巧妙的简化。因为每个“组”的奖励要么是 0，要么是
1，这构成了一个<strong>伯努利分布（Bernoulli
distribution）</strong>。</li>
<li><span class="math inline">$\overline{R}^{group}$</span>
就是这个分布的“成功概率”（即抽到一个正面组的概率）。</li>
<li>对于伯努利分布，标准差 <span class="math inline">$\sigma = \sqrt{p
\times (1-p)}$</span>，其中 <span class="math inline"><em>p</em></span>
是成功概率。</li>
<li>因此，<span class="math inline">$\sigma^{group} =
\sqrt{\overline{R}^{group} \times (1 -
\overline{R}^{group})}$</span>。</li>
</ul></li>
</ul>
<p><strong>公式 (13) <span
class="math inline"><em>Â</em><sub><em>p</em><em>o</em><em>s</em></sub><sup><em>g</em><em>r</em><em>o</em><em>u</em><em>p</em></sup></span>
和 <span
class="math inline"><em>Â</em><sub><em>n</em><em>e</em><em>g</em></sub><sup><em>g</em><em>r</em><em>o</em><em>u</em><em>p</em></sup></span>
(组的理论优势)</strong></p>
<p><span class="math display">$$
\hat{A}_{pos}^{group}=\frac{1-R^{group}}{\sigma^{group}}
\quad
\hat{A}_{neg}^{group}=-\frac{\overline{R}^{group}}{\sigma^{group}}
$$</span></p>
<ul>
<li><strong>含义：</strong> 这就是我们熟悉的“优势”z-score 公式 。
<ul>
<li>一个“正面组”的优势 = (它的奖励 1 - 平均奖励 <span
class="math inline">$\overline{R}^{group}$</span>) / 标准差 <span
class="math inline"><em>σ</em><sup><em>g</em><em>r</em><em>o</em><em>u</em><em>p</em></sup></span>。</li>
<li>一个“负面组”的优势 = (它的奖励 0 - 平均奖励 <span
class="math inline">$\overline{R}^{group}$</span>) / 标准差 <span
class="math inline"><em>σ</em><sup><em>g</em><em>r</em><em>o</em><em>u</em><em>p</em></sup></span>。</li>
</ul></li>
</ul>
<h4
id="步骤三计算单个响应的精确优势公式-14-15">步骤三：计算“单个响应”的精确优势（公式
14-15）</h4>
<p>这是最后一步，也是最关键的一步。我们不想知道“组”的优势，我们想知道“<strong>单个响应</strong>”
<span class="math inline"><em>ŷ</em><sub><em>i</em></sub></span>
的优势。</p>
<ul>
<li><strong>核心逻辑：</strong> <span
class="math inline"><em>ŷ</em><sub><em>i</em></sub></span>
的“解析优势”，等于它<strong>可能参与</strong>的<strong>所有理论组</strong>的<strong>平均优势</strong>。</li>
<li><strong><span
class="math inline"><em>ŷ</em><sub><em>i</em></sub></span>
会参与多少个组？</strong>
<ul>
<li>一个组有 <span class="math inline"><em>k</em></span>
个成员。如果我们<strong>固定</strong> <span
class="math inline"><em>ŷ</em><sub><em>i</em></sub></span>
为其中一个成员，我们还需要从<strong>剩下</strong>的 <span
class="math inline"><em>N</em><sub><em>r</em><em>o</em><em>l</em><em>l</em><em>o</em><em>u</em><em>t</em></sub> − 1</span>
个响应中，再挑选 <span class="math inline"><em>k</em> − 1</span>
个“伙伴”。</li>
<li>因此，<span
class="math inline"><em>ŷ</em><sub><em>i</em></sub></span> 总共会参与
<span class="math inline">$\binom{N_{rollout}-1}{k-1}$</span>
个不同的理论组。</li>
</ul></li>
</ul>
<p><strong>公式 (14) <span
class="math inline"><em>Â</em><sub><em>p</em><em>o</em><em>s</em></sub></span>
(一个“正面响应”的优势)</strong></p>
<p><span class="math display">$$
\hat{A}_{pos}=\frac{1-\overline{R}^{group}}{\sigma^{group}}
$$</span></p>
<ul>
<li><strong>含义：</strong> 这是一个<strong>正确响应</strong>（<span
class="math inline"><em>ŷ</em><sub><em>i</em></sub></span> 的 <span
class="math inline"><em>R</em><sub><em>i</em></sub> = 1</span>）的解析优势
。</li>
<li><strong>推导（参见附录 B，公式 28-29 ）：</strong>
<ul>
<li>如果 <span
class="math inline"><em>ŷ</em><sub><em>i</em></sub></span>
是一个“正面响应”，那么<strong>无论</strong>它和谁（那 <span
class="math inline"><em>k</em> − 1</span>
个伙伴）组合，这个组都<strong>必然</strong>是一个“正面组”（因为它至少包含了
<span class="math inline"><em>ŷ</em><sub><em>i</em></sub></span> 这 1
个正确响应）。</li>
<li>因此，<span
class="math inline"><em>ŷ</em><sub><em>i</em></sub></span> 参与的所有
<span class="math inline">$\binom{N_{rollout}-1}{k-1}$</span>
个组<strong>全部都是正面组</strong>。</li>
<li>它的平均优势，自然就等于 <span
class="math inline"><em>Â</em><sub><em>p</em><em>o</em><em>s</em></sub><sup><em>g</em><em>r</em><em>o</em><em>u</em><em>p</em></sup></span>。</li>
</ul></li>
</ul>
<p><strong>公式 (15) <span
class="math inline"><em>Â</em><sub><em>n</em><em>e</em><em>g</em></sub></span>
(一个“负面响应”的优势)</strong></p>
<p><span class="math display">$$
\hat{A}_{neg}=\frac{1-\overline{R}^{group}-\dfrac{\binom{N_{neg}-1}{k-1}}{\binom{N_{rollout}-1}{k-1}}}{\sigma^{group}}
$$</span></p>
<ul>
<li><strong>含义：</strong> 这是一个<strong>错误响应</strong>（<span
class="math inline"><em>ŷ</em><sub><em>i</em></sub></span> 的 <span
class="math inline"><em>R</em><sub><em>i</em></sub> = 0</span>）的解析优势
。</li>
<li><strong>推导（参见附录 B，公式 30-33 ）：</strong>
<ul>
<li><p>这是最复杂但也是最精妙的部分。如果 <span
class="math inline"><em>ŷ</em><sub><em>i</em></sub></span>
是一个“负面响应”，那么它参与的 <span
class="math inline">$\binom{N_{rollout}-1}{k-1}$</span>
个组，既可能是“正面组”，也可能是“负面组”。</p></li>
<li><p><strong><span
class="math inline"><em>ŷ</em><sub><em>i</em></sub></span>
会参与多少个“负面组”？</strong></p>
<ul>
<li><span class="math inline"><em>ŷ</em><sub><em>i</em></sub></span>
要组成一个“负面组”，它的 <span class="math inline"><em>k</em> − 1</span>
个伙伴<strong>必须</strong>也都是负面响应。</li>
<li>总共有 <span
class="math inline"><em>N</em><sub><em>n</em><em>e</em><em>g</em></sub></span>
个负面响应。除开 <span
class="math inline"><em>ŷ</em><sub><em>i</em></sub></span>
自己，还<strong>剩下</strong> <span
class="math inline"><em>N</em><sub><em>n</em><em>e</em><em>g</em></sub> − 1</span>
个负面响应。</li>
<li>因此，<span
class="math inline"><em>ŷ</em><sub><em>i</em></sub></span>
参与的“负面组”数量为 <span
class="math inline">$\binom{N_{neg}-1}{k-1}$</span>。</li>
</ul></li>
<li><p><strong><span
class="math inline"><em>ŷ</em><sub><em>i</em></sub></span>
会参与多少个“正面组”？</strong></p>
<ul>
<li>数量 = (它参与的总组数) - (它参与的负面组数) = <span
class="math inline">$\binom{N_{rollout}-1}{k-1} -
\binom{N_{neg}-1}{k-1}$</span>。</li>
</ul></li>
<li><p><strong><span
class="math inline"><em>Â</em><sub><em>n</em><em>e</em><em>g</em></sub></span>
的计算（加权平均）：</strong> <span
class="math inline"><em>Â</em><sub><em>n</em><em>e</em><em>g</em></sub></span>
= <span class="math inline">$\frac{\text{(正面组数)} \times
\hat{A}_{pos}^{group} + \text{(负面组数)} \times
\hat{A}_{neg}^{group}}{\text{总组数}}$</span></p>
<p><span class="math inline">$\hat{A}_{neg} =
\frac{(\binom{N_{rollout}-1}{k-1} -
\binom{N_{neg}-1}{k-1})\hat{A}_{pos}^{group} +
\binom{N_{neg}-1}{k-1}\hat{A}_{neg}^{group}}{\binom{N_{rollout}-1}{k-1}}$</span></p></li>
<li><p>将公式 (13) 代入上式，经过一系列代数化简（附录 B
中已完成），即可得到最终的公式 (15)。</p></li>
</ul></li>
</ul>
<p><strong>总结：</strong> 通过公式 (14) 和
(15)，我们现在有了一个<strong>完全不需要采样</strong>的、<strong>确定性</strong>的计算方法：对于一个问题
<span class="math inline"><em>x</em></span>，我们只需 <span
class="math inline"><em>N</em><sub><em>r</em><em>o</em><em>l</em><em>l</em><em>o</em><em>u</em><em>t</em></sub></span>
次，数出 <span
class="math inline"><em>N</em><sub><em>p</em><em>o</em><em>s</em></sub></span>
和 <span
class="math inline"><em>N</em><sub><em>n</em><em>e</em><em>g</em></sub></span>，代入公式，就能立刻得到<strong>所有</strong>正面响应和<strong>所有</strong>负面响应的<strong>精确优势值</strong>。</p>
<p><strong>图 5 的实证（Figure 5）</strong></p>
<p>如图 5 所示 ，解析推导（蓝色）<strong>消除了</strong> Bootstrap
采样（紫色）的训练波动，带来了<strong>最稳定、最持续的性能提升</strong>。</p>
<hr />
<h2
id="第三部分实证分析passk-训练为何有效第3节">第三部分：实证分析：Pass@k
训练为何有效（第3节）</h2>
<p>第 3 节通过一系列的“控制变量实验”来回答：Pass@k
训练的有效性背后的“机制”到底是什么。</p>
<h3 id="对比实验为什么-passk-优于其他探索机制第3.1节">3.1
对比实验：为什么 Pass@k 优于其他探索机制？（第3.1节）</h3>
<p>Pass@k
的成功是因为它鼓励探索，那么其他探索方法（如增加噪音或熵）是否也有效？</p>
<ul>
<li><p><strong>对比“噪音奖励”（Noise Rewards，图 6a）：</strong></p>
<ul>
<li>实验：简单地将一定比例（10%, 30%, 50%）的负面奖励（0
分）随机“翻转”为正面奖励（1 分）。</li>
<li>结果：如图 6a 所示
，这<strong>严重损害</strong>了模型性能。噪音比例越高，性能下降越快。</li>
<li><strong>结论：</strong> Pass@k
训练<strong>不是</strong>简单的“增加噪音”。它是一种<strong>有结构的（structured）</strong>、<strong>有原则的（principled）</strong>探索。它不是“随机”奖励错误答案，而是“有条件地”奖励那些（虽然错误但）与某个“正确答案”共同出现在一个组里的探索。</li>
</ul></li>
<li><p><strong>对比“熵正则化”（Entropy Regularization，图
6b）：</strong></p>
<ul>
<li>实验：“熵”是衡量模型输出“不确定性”或“多样性”的指标。熵正则化是在目标函数（公式
4）中加入一个“熵奖励”，鼓励模型保持“不确定性”。</li>
<li>结果：如图 6b 所示
，熵正则化（绿色、红色、黄色）效果不佳，高系数的熵（0.005）甚至直接导致模型“崩溃”。</li>
<li><strong>结论：</strong>
强行“要求”模型保持不确定性，会与“要求”模型找到正确答案的（Pass@1）目标相冲突，导致训练不稳定。Pass@k
则更自然。</li>
</ul></li>
</ul>
<h3 id="机制分析passk-如何真正提升探索能力第3.2节">3.2 机制分析：Pass@k
如何真正提升探索能力？（第3.2节）</h3>
<p>本节通过两个关键指标（答案多样性、策略熵）来“打开黑盒”，观察 Pass@k
训练期间发生了什么 。</p>
<ul>
<li><p><strong>图 7a：负面响应的答案多样性 (Answer Diversity of Negative
Responses)</strong></p>
<ul>
<li><strong>Pass@1 训练（绿色）：</strong>
多样性一直保持在低水平。这说明模型陷入了“局部最优”——它找到了一个“最喜欢”的<strong>错误答案</strong>，并<strong>固执地</strong>重复它。</li>
<li><strong>Pass@k 训练（蓝色）：</strong>
多样性持续保持在<strong>高水平</strong>。这说明，当模型不自信时，Pass@k
训练激励它去尝试<strong>各种不同</strong>的解决方案。</li>
<li><strong>结论：</strong> Pass@k
训练名副其实地<strong>提升了模型探索的广度</strong>。</li>
</ul></li>
<li><p><strong>图 7b：策略熵 (Entropy of Policy
Distribution)</strong></p>
<ul>
<li><strong>Pass@1 训练（红色）：</strong>
策略熵（不确定性）迅速<strong>下降</strong>。这表明模型对它的（次优）答案变得“过度自信”，<strong>失去了</strong>探索能力。</li>
<li><strong>Pass@k 训练（蓝色）：</strong>
策略熵<strong>保持在较高水平</strong>。这表明模型在训练过程中<strong>保留了</strong>其探索能力。</li>
<li><strong>结论：</strong> Pass@k 训练（蓝色）在提升 Pass@1 分数（图 7a
中的蓝色“Accuracy”）的同时，保持了高熵（图 7b
的蓝色“Entropy”）。这在视觉上证明了论文的核心假说：<strong>探索和利用可以相互促进</strong>。Pass@1
训练（红色）为了提升 Pass@1 分数（图 7a
的绿色“Accuracy”），<strong>牺牲了</strong>熵（图 7b
的红色“Entropy”），导致探索能力丧失，最终的 Pass@1
性能也受限于局部最优。</li>
</ul></li>
</ul>
<h3 id="泛化与鲁棒性第3.3节与第3.4节">3.3
泛化与鲁棒性（第3.3节与第3.4节）</h3>
<ul>
<li><strong>泛化性（表 1）：</strong> 相比 Pass@1 训练，Pass@k
训练不仅在“域内”（In-Domain，训练过的任务）提升更大，在“域外”（Out-of-Domain，未见过的任务）上<strong>泛化能力也更强</strong>
。这是因为 Pass@k
鼓励的“探索”让模型学到了更广泛、更鲁棒的知识，而不仅仅是“记住”训练集的最优路径。</li>
<li><strong>鲁棒性（图 8）：</strong> Pass@k 训练对 <span
class="math inline"><em>k</em></span> 值的选择（<span
class="math inline"><em>k</em> = 4, 8, 16</span>）是<strong>鲁棒的</strong>
。如图 8a 和 8b 所示，所有 <span class="math inline"><em>k</em></span>
值都带来了性能提升。唯一的区别是，<span
class="math inline"><em>k</em></span> 值越大（如 <span
class="math inline"><em>k</em> = 16</span>），收敛速度越慢。但作者在图
8c 和 8d
中证明，这个问题可以通过简单地“调高学习率（LR）”来轻松解决。</li>
</ul>
<h3 id="最终实证王牌策略与-sota-性能第3.5节">3.4 最终实证：“王牌”策略与
SOTA 性能（第3.5节）</h3>
<p>这是整篇论文的“高光时刻”。作者提出了一个“王牌”训练策略，并用它实现了
SOTA（State-of-the-Art）级别的性能 。</p>
<ul>
<li><p><strong>“王牌”策略：P@k T. + P@1 T.</strong></p>
<ol type="1">
<li><strong>第一阶段 (P@k T.)：</strong> 先用 <strong>Pass@k
训练</strong>。此阶段的<strong>唯一</strong>目的，是利用 Pass@k
的探索性，将模型<strong>“拽出”</strong>它原来的“局部最优”陷阱，为模型打开一个更广阔、潜力更高的解空间。</li>
<li><strong>第二阶段 (P@1 T.)：</strong>
在第一阶段的基础上，再用<strong>Pass@1
训练</strong>。此阶段的<strong>唯一</strong>目的，是在这个“新发现的”广阔空间里，利用
Pass@1
的“利用性”，<strong>“打磨”</strong>出那个最精确、最自信的“全局最优”解。</li>
</ol></li>
<li><p><strong>表 2：Enigmata 任务上的惊人结果 </strong></p>
<ul>
<li>这是一个基于 Qwen2.5-7B（一个 70 亿参数的开源模型）的实验。</li>
<li><strong>基线（Baseline）：</strong> 4.7% 准确率。</li>
<li><strong>仅 P@1 T.：</strong> 12.9% 准确率（陷入局部最优）。</li>
<li><strong>仅 P@k T.：</strong> 17.9%
准确率（探索有余，打磨不足）。</li>
<li><strong>“P@k T. + P@1 T.” 策略：</strong> 达到了
<strong>30.8%</strong> 的准确率！</li>
<li><strong>SOTA 对比：</strong> 这个 <strong>30.8%</strong>
的成绩，<strong>超越了</strong>当时所有强大的闭源模型，包括 Grok-2
(13.6%), GPT-40-1120 (14.2%), 和 Claude-3.7-Sonnet (22.7%)。</li>
</ul></li>
<li><p><strong>表 3：多模态任务上的验证 </strong></p>
<ul>
<li>在包含图像的 MathVision 和 MMMU
任务上，该策略（64.4%）同样优于单独的 P@1 T. (63.7%) 或 P@k T.
(63.0%)。</li>
</ul></li>
<li><p><strong>最终结论：</strong>
这雄辩地证明了论文的核心假说。<strong>探索（Pass@k）不是目的，而是实现更优“利用”（Pass@1）的必要手段。</strong></p></li>
</ul>
<table>
<colgroup>
<col style="width: 34%" />
<col style="width: 25%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">训练策略</th>
<th style="text-align: left;">Enigmata 总体准确率 (Pass@1)</th>
<th style="text-align: left;">结论</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>闭源 SOTA</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Grok-2-1212</td>
<td style="text-align: left;">13.6%</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-40-1120</td>
<td style="text-align: left;">14.2%</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Claude-3.7-Sonnet</td>
<td style="text-align: left;">22.7%</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td
style="text-align: left;"><strong>Qwen2.5-7B（本文模型）</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">基线（Baseline）</td>
<td style="text-align: left;">4.7%</td>
<td style="text-align: left;">模型的初始状态</td>
</tr>
<tr>
<td style="text-align: left;">+ P@1 T. (传统方法)</td>
<td style="text-align: left;">12.9%</td>
<td style="text-align: left;">陷入局部最优，性能受限</td>
</tr>
<tr>
<td style="text-align: left;">+ P@k T. (仅探索)</td>
<td style="text-align: left;">17.9%</td>
<td style="text-align: left;">探索了新空间，但未充分利用</td>
</tr>
<tr>
<td style="text-align: left;"><strong>+ P@k T. + P@1 T.
(王牌策略)</strong></td>
<td style="text-align: left;"><strong>30.8%</strong></td>
<td style="text-align: left;"><strong>通过探索实现了更优的利用，超越
SOTA</strong></td>
</tr>
</tbody>
</table>
<hr />
<h2
id="第四部分理论升华隐式奖励设计第4节">第四部分：理论升华：“隐式奖励设计”（第4节）</h2>
<p>如果说第 2 节是“如何做”，第 3 节是“有没有效”，那么第 4
节就是“为什么有效”的理论升华。作者从 Pass@k
训练的成功中，提炼出了一个更深刻、更具普遍性的概念。</p>
<h3 id="深入分析pass1-和-passk-的-eta-曲线第4.1节">4.1 深入分析：Pass@1
和 Pass@k 的 <span class="math inline"><em>η</em></span>
曲线（第4.1节）</h3>
<p>作者发现，Pass@k 训练（解析推导版）之所以有效，关键在于其“优势函数”
<span class="math inline"><em>Â</em></span> 的<strong>形状</strong>
。</p>
<p>为了分析这个“形状”，作者定义了一个新指标 <span
class="math inline"><em>η</em></span>。</p>
<p><strong>公式 (16) <span class="math inline"><em>η</em></span>
(绝对优势总和)</strong></p>
<p><span
class="math display"><em>η</em> = <em>N</em><sub><em>P</em><em>o</em><em>s</em></sub> × |<em>A</em><sub><em>p</em><em>o</em><em>s</em></sub>|+<em>N</em><sub><em>n</em><em>e</em><em>g</em></sub> × |<em>A</em><sub><em>n</em><em>e</em><em>g</em></sub>|</span></p>
<ul>
<li><strong>含义：</strong> <span class="math inline"><em>η</em></span>
(eta) 被定义为“<strong>绝对优势总和</strong>”（Sum of Absolute
Advantage）。</li>
<li><strong>拆解：</strong>
它是一个问题在当前状态下的“<strong>总优化强度</strong>”的度量。
<ul>
<li><span
class="math inline">|<em>A</em><sub><em>p</em><em>o</em><em>s</em></sub>|</span>
是一个正面响应的优势<strong>大小</strong>。</li>
<li><span
class="math inline">|<em>A</em><sub><em>n</em><em>e</em><em>g</em></sub>|</span>
是一个负面响应的优势<strong>大小</strong>。</li>
<li><span class="math inline"><em>η</em></span> =
(所有正面响应的优势大小总和) + (所有负面响应的优势大小总和)。</li>
</ul></li>
<li><strong>作用：</strong> <span class="math inline"><em>η</em></span>
越大，意味着模型在这个问题上受到的“驱动力”或“惩罚”越大，策略更新的幅度也越大。</li>
</ul>
<p><strong>图 9：<span class="math inline"><em>η</em></span>
曲线的“Aha!”时刻 </strong></p>
<p>图 9
是理解本篇论文“理论贡献”<strong>最重要</strong>的一张图。它绘制了 <span
class="math inline"><em>η</em></span>（总优化强度）作为“Rollout
准确率”（即 <span
class="math inline"><em>N</em><sub><em>p</em><em>o</em><em>s</em></sub>/<em>N</em><sub><em>r</em><em>o</em><em>l</em><em>l</em><em>o</em><em>u</em><em>t</em></sub></span>）的函数：</p>
<ul>
<li><strong>Pass@1 训练（图 9a）：</strong>
<ul>
<li><strong>形状：</strong> <span class="math inline"><em>η</em></span>
曲线是<strong>对称的</strong>，在准确率为 50% 时达到峰值。</li>
<li><strong>问题：</strong> 这意味着，Pass@1
训练在“简单问题”（例如准确率
80%）上花费的优化力气，和在“难题”（例如准确率
20%）上花费的力气<strong>几乎一样多</strong>。这导致模型<strong>浪费</strong>了大量的优化精力去“过度拟合”那些它已经基本掌握的简单问题，从而陷入局部最优。</li>
</ul></li>
<li><strong>Pass@k 训练（图 9b）：</strong>
<ul>
<li><strong>形状：</strong> <span class="math inline"><em>η</em></span>
曲线是<strong>高度不对称的</strong>。</li>
<li><strong>Argmax（峰值）：</strong> <span
class="math inline"><em>η</em></span>
的峰值（Argmax）出现在<strong>低准确率区域</strong>（约 25%）。这意味着
Pass@k
训练<strong>自动地</strong>将最强的优化火力集中在<strong>“难题区”</strong>（即那些模型不擅长、但又不是完全没希望的问题）。</li>
<li><strong>Trend（趋势）：</strong>
随着准确率的提高（问题变“简单”），<span
class="math inline"><em>η</em></span> 值<strong>迅速下降趋向于
0</strong>。这意味着 Pass@k
训练<strong>自动地学会了“忽略”</strong>那些它已经掌握的简单问题，从而避免了过度拟合。</li>
</ul></li>
</ul>
<p><strong>结论：</strong> Pass@k
训练（解析推导版）不仅仅是“鼓励探索”，它本质上是一个<strong>“自动课程学习”（Automatic
Curriculum）</strong>机制。它天生就会<strong>“关注难题，忽略易题”</strong>，这就是它能逃离局部最优、持续提升性能的根本原因。</p>
<h3 id="概念升华隐式奖励设计第4.2节">4.2
概念升华：“隐式奖励设计”（第4.2节）</h3>
<p>基于 4.1
节的深刻发现，作者提出了整篇论文的“理论升华”：<strong>隐式奖励设计（Implicit
Reward Design）</strong>。</p>
<ul>
<li><strong>传统设计（显式奖励）：</strong> 传统 RLVR
的思路是设计“奖励函数 <span
class="math inline"><em>R</em></span>”（例如，是 0/1 还是 0.5/1）。</li>
<li><strong>本文的启示（隐式设计）：</strong> Pass@k
的成功告诉我们，我们不必纠结于 <span
class="math inline"><em>R</em></span>，我们可以<strong>“直接设计优势函数
<span class="math inline"><em>Â</em></span>
的形状”</strong>（例如，设计一个像图 9b 那样不对称的 <span
class="math inline"><em>η</em></span> 曲线）。</li>
<li><strong>定义：</strong>
“隐式奖励设计”就是指，通过直接设计（或选择，如
Pass@k）具有特定形状（例如，“关注难题”）的优势函数 <span
class="math inline"><em>Â</em></span>，来<strong>间接（Implicitly）</strong>地引导模型的优化方向。</li>
</ul>
<p>作者在 4.2 节中展示了这种新“设计范式”的威力，提出了几个 Pass@k
的“魔改”版本：</p>
<ul>
<li><p><strong>“超越 Pass@k” (Exceeding Pass@k, 公式 17)：</strong>
通过一个变换函数 <span
class="math inline"><em>f</em>(<em>N</em><sub><em>p</em><em>o</em><em>s</em></sub>)</span>，人为地将
<span class="math inline"><em>η</em></span>
曲线的峰值“推向”更难的区域，以“夸大”Pass@k 的效果 。</p></li>
<li><p><strong>“组合训练” (Combination Training, 公式 18)：</strong></p>
<p><span class="math display">$$
\hat{A}=\frac{N_{pos}}{N}\times\hat{A}_{Pass@k}+(1-\frac{N_{pos}}{N})\times\hat{A}_{Pass@1}
$$</span></p>
<ul>
<li><strong>含义：</strong> 这是一个动态的“混合”优势函数 。</li>
<li><strong>逻辑：</strong> 当准确率 <span
class="math inline"><em>N</em><sub><em>p</em><em>o</em><em>s</em></sub>/<em>N</em></span>
很<strong>低</strong>时（难题），权重偏向 <span
class="math inline"><em>Â</em><sub><em>P</em><em>a</em><em>s</em><em>s</em>@1</sub></span>（利用）；当准确率很<strong>高</strong>时（易题），权重偏向
<span
class="math inline"><em>Â</em><sub><em>P</em><em>a</em><em>s</em><em>s</em>@<em>k</em></sub></span>（Pass@k
的 <span class="math inline"><em>η</em></span> 曲线在易题区接近
0，能防止过拟合）。如图 12 所示，这个组合策略比单独的 Pass@k
效果更好。</li>
</ul></li>
<li><p><strong>“自适应训练” (Adaptive Training)：</strong>
使用“熵”作为<strong>信号</strong>（而不是像 3.1
节那样作为奖励）。对于“低熵”（过度自信）的问题，强行使用 Pass@k
优势函数来“逼迫”模型探索；对于“高熵”（已经在探索）的问题，使用 Pass@1
优势函数来“收割”探索的成果 。</p></li>
</ul>
<hr />
<h2
id="第五部分总结与展望第5节与第6节">第五部分：总结与展望（第5节与第6节）</h2>
<h3 id="相关工作第5节">5.1 相关工作（第5节）</h3>
<p>第 5 节将本文的工作置于更广阔的学术背景中 。</p>
<ul>
<li><strong>RLVR 训练：</strong> 本文是 RLVR 训练范式（如 DeepSeek-R1
）的直接继承和重大改进。</li>
<li><strong>探索机制：</strong>
相比于现有的探索方法，如“测试时扩展”（Test-time
Scaling）或“熵正则化”（Entropy Regularization），本文的 Pass@k
训练（尤其是在 3.1
节中被证明）是一种更稳定、更有效、更具原则性的探索机制。</li>
</ul>
<h3 id="结论第6节">5.2 结论（第6节）</h3>
<p>这篇论文提供了两个层面的核心贡献 ：</p>
<ol type="1">
<li><strong>一个实用的“工具”：</strong>
论文提供了一个具体、可行且效果惊人的训练策略，即 <strong>“P@k T. + P@1
T.”（先 Pass@k 探索，再 Pass@1
利用）</strong>。这是一个您可以直接在自己研究中借鉴或使用的“算法工具”，它已经被证明（表
2）能让一个 7B 的模型击败 GPT-40 和 Claude-3.7。</li>
<li><strong>一个深刻的“思想”：</strong>
论文提炼了<strong>“隐式奖励设计”</strong>这一理论概念。它开辟了一个新的研究方向：即强化学习的优化不应只关注“设计奖励
<span
class="math inline"><em>R</em></span>”，更应该关注“<strong>设计优势函数
<span class="math inline"><em>Â</em></span>
的形状</strong>”。这个“思想”比“工具”更重要，它为您（以及这个领域）指明了未来可能的研究方向，即如何通过更精细地控制优化过程（finer-grained
control over optimization）来释放模型的全部潜力 。</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://kjore.github.io">kjore</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://kjore.github.io/2025/11/14/%E7%94%A8%E4%BA%8E%E8%87%AA%E9%80%82%E5%BA%94%E5%B9%B3%E8%A1%A1%E5%A4%A7%E5%9E%8B%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%A9%E7%94%A8%E7%9A%84%20Pass@k%20%E8%AE%AD%E7%BB%83/">https://kjore.github.io/2025/11/14/%E7%94%A8%E4%BA%8E%E8%87%AA%E9%80%82%E5%BA%94%E5%B9%B3%E8%A1%A1%E5%A4%A7%E5%9E%8B%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%A9%E7%94%A8%E7%9A%84%20Pass@k%20%E8%AE%AD%E7%BB%83/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://kjore.github.io" target="_blank">kjore's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/">论文研读</a></div><div class="post-share"><div class="social-share" data-image="/img/head.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/11/14/DARLING/" title="《共同加强语言模型生成的多样性和质量》论文深度研读"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">《共同加强语言模型生成的多样性和质量》论文深度研读</div></div><div class="info-2"><div class="info-item-1">论文链接：https://arxiv.org/abs/2509.02534 深度解析：DARLING论文《联合增强语言模型生成的“多样性”与“质量”》 导言：现代大语言模型的核心困境 大型语言模型（LLMs）的后训练（Post-training）过程，例如基于人类反馈的强化学习（RLHF），显著提升了模型的准确性和实用性。然而，这种优化带来了一个严重的副作用：模型的多样性（diversity）大幅下降 。 在抽象（Abstract）和引言（Introduction）部分（第1节），论文指出了当前方法的核心矛盾：为了追求质量，模型被训练得“过度锐化”（overly sharpened），其输出分布变得极窄 。这意味着，模型倾向于为同一个提示（prompt）生成高度相似、甚至近乎重复的答案。这种“多样性崩塌”（diversity collapse）现象，极大地限制了 LLMs 在需要创意和探索性任务（如头脑风暴、讲故事或解决复杂问题）中的应用价值 。 为解决这一挑战，研究人员提出了 DARLING (Diversity-Aware Reinforcement ...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/11/14/DARLING/" title="《共同加强语言模型生成的多样性和质量》论文深度研读"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-14</div><div class="info-item-2">《共同加强语言模型生成的多样性和质量》论文深度研读</div></div><div class="info-2"><div class="info-item-1">论文链接：https://arxiv.org/abs/2509.02534 深度解析：DARLING论文《联合增强语言模型生成的“多样性”与“质量”》 导言：现代大语言模型的核心困境 大型语言模型（LLMs）的后训练（Post-training）过程，例如基于人类反馈的强化学习（RLHF），显著提升了模型的准确性和实用性。然而，这种优化带来了一个严重的副作用：模型的多样性（diversity）大幅下降 。 在抽象（Abstract）和引言（Introduction）部分（第1节），论文指出了当前方法的核心矛盾：为了追求质量，模型被训练得“过度锐化”（overly sharpened），其输出分布变得极窄 。这意味着，模型倾向于为同一个提示（prompt）生成高度相似、甚至近乎重复的答案。这种“多样性崩塌”（diversity collapse）现象，极大地限制了 LLMs 在需要创意和探索性任务（如头脑风暴、讲故事或解决复杂问题）中的应用价值 。 为解决这一挑战，研究人员提出了 DARLING (Diversity-Aware Reinforcement ...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/head.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">kjore</div><div class="author-info-description">什么都不懂</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/kjore"><i class="fab fa-github"></i><span>关注我</span></a><div class="card-info-social-icons"><a class="social-icon" href="mailto:zhangjingye@bupt.edu.cn" target="_blank" title="邮箱"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#passk-%E8%AE%AD%E7%BB%83%E8%AE%BA%E6%96%87%E6%B7%B1%E5%BA%A6%E7%A0%94%E8%AF%BB"><span class="toc-number">1.</span> <span class="toc-text">《Pass@k 训练》论文深度研读</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E9%97%AE%E9%A2%98%E7%9A%84%E6%8F%90%E5%87%BA%E6%91%98%E8%A6%81%E4%B8%8E%E7%AC%AC1%E8%8A%82%E5%BC%95%E8%A8%80"><span class="toc-number">1.1.</span> <span class="toc-text">第一部分：问题的提出（摘要与第1节：引言）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E5%9C%BA%E6%99%AFrlvr-%E4%B8%8E%E5%A4%A7%E5%9E%8B%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1 核心场景：RLVR
与大型推理模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95%E7%9A%84%E5%9B%B0%E5%A2%83pass1-%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%B1%80%E9%83%A8%E6%9C%80%E4%BC%98%E9%99%B7%E9%98%B1"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2
传统方法的困境：Pass@1 训练与“局部最优”陷阱</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E7%9A%84%E6%A0%B8%E5%BF%83%E6%96%B9%E6%A1%88passk-%E8%AE%AD%E7%BB%83"><span class="toc-number">1.1.3.</span> <span class="toc-text">1.3 论文的核心方案：Pass@k 训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E5%81%87%E8%AF%B4%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%88%A9%E7%94%A8%E7%9A%84%E7%9B%B8%E4%BA%92%E5%A2%9E%E5%BC%BA"><span class="toc-number">1.1.4.</span> <span class="toc-text">1.4
核心假说：探索与利用的“相互增强”</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6%E5%B0%86-passk-%E5%AE%9E%E7%8E%B0%E4%B8%BA%E5%A5%96%E5%8A%B1%E7%AC%AC2%E8%8A%82"><span class="toc-number">1.2.</span> <span class="toc-text">第二部分：技术框架：将
Pass@k 实现为奖励（第2节）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E7%BA%BF%E5%9B%9E%E9%A1%BEpass1-%E8%AE%AD%E7%BB%83%E7%9A%84%E6%95%B0%E5%AD%A6%E8%A1%A8%E8%BF%B0%E7%AC%AC2.1%E8%8A%82"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 基线回顾：Pass@1
训练的数学表述（第2.1节）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E5%AE%9E%E7%8E%B0passk-%E8%AE%AD%E7%BB%83%E4%B9%8B%E5%85%A8%E9%87%87%E6%A0%B7%E7%AC%AC2.2%E8%8A%82"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 朴素实现：Pass@k
训练之“全采样”（第2.2节）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AB%98%E6%95%88%E5%AE%9E%E7%8E%B0passk-%E8%AE%AD%E7%BB%83%E4%B9%8Bbootstrap-%E9%87%87%E6%A0%B7%E7%AC%AC2.3%E8%8A%82"><span class="toc-number">1.2.3.</span> <span class="toc-text">2.3 高效实现：Pass@k
训练之“Bootstrap 采样”（第2.3节）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%88%E6%9E%81%E5%AE%9E%E7%8E%B0passk-%E8%AE%AD%E7%BB%83%E4%B9%8B%E8%A7%A3%E6%9E%90%E6%8E%A8%E5%AF%BC%E7%AC%AC2.4%E8%8A%82%E4%B8%8E%E9%99%84%E5%BD%95b"><span class="toc-number">1.2.4.</span> <span class="toc-text">2.4 终极实现：Pass@k
训练之“解析推导”（第2.4节与附录B）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4%E4%B8%80%E7%BB%9F%E8%AE%A1%E6%89%80%E6%9C%89%E5%8F%AF%E8%83%BD%E7%9A%84%E7%BB%84%E5%85%AC%E5%BC%8F-7-10"><span class="toc-number">1.2.4.1.</span> <span class="toc-text">步骤一：统计所有可能的“组”（公式
7-10）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4%E4%BA%8C%E8%AE%A1%E7%AE%97%E7%BB%84%E7%9A%84%E7%B2%BE%E7%A1%AE%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E5%85%AC%E5%BC%8F-11-13"><span class="toc-number">1.2.4.2.</span> <span class="toc-text">步骤二：计算“组”的精确统计数据（公式
11-13）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4%E4%B8%89%E8%AE%A1%E7%AE%97%E5%8D%95%E4%B8%AA%E5%93%8D%E5%BA%94%E7%9A%84%E7%B2%BE%E7%A1%AE%E4%BC%98%E5%8A%BF%E5%85%AC%E5%BC%8F-14-15"><span class="toc-number">1.2.4.3.</span> <span class="toc-text">步骤三：计算“单个响应”的精确优势（公式
14-15）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E5%AE%9E%E8%AF%81%E5%88%86%E6%9E%90passk-%E8%AE%AD%E7%BB%83%E4%B8%BA%E4%BD%95%E6%9C%89%E6%95%88%E7%AC%AC3%E8%8A%82"><span class="toc-number">1.3.</span> <span class="toc-text">第三部分：实证分析：Pass@k
训练为何有效（第3节）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E6%AF%94%E5%AE%9E%E9%AA%8C%E4%B8%BA%E4%BB%80%E4%B9%88-passk-%E4%BC%98%E4%BA%8E%E5%85%B6%E4%BB%96%E6%8E%A2%E7%B4%A2%E6%9C%BA%E5%88%B6%E7%AC%AC3.1%E8%8A%82"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1
对比实验：为什么 Pass@k 优于其他探索机制？（第3.1节）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90passk-%E5%A6%82%E4%BD%95%E7%9C%9F%E6%AD%A3%E6%8F%90%E5%8D%87%E6%8E%A2%E7%B4%A2%E8%83%BD%E5%8A%9B%E7%AC%AC3.2%E8%8A%82"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 机制分析：Pass@k
如何真正提升探索能力？（第3.2节）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%9B%E5%8C%96%E4%B8%8E%E9%B2%81%E6%A3%92%E6%80%A7%E7%AC%AC3.3%E8%8A%82%E4%B8%8E%E7%AC%AC3.4%E8%8A%82"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.3
泛化与鲁棒性（第3.3节与第3.4节）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E7%BB%88%E5%AE%9E%E8%AF%81%E7%8E%8B%E7%89%8C%E7%AD%96%E7%95%A5%E4%B8%8E-sota-%E6%80%A7%E8%83%BD%E7%AC%AC3.5%E8%8A%82"><span class="toc-number">1.3.4.</span> <span class="toc-text">3.4 最终实证：“王牌”策略与
SOTA 性能（第3.5节）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E7%90%86%E8%AE%BA%E5%8D%87%E5%8D%8E%E9%9A%90%E5%BC%8F%E5%A5%96%E5%8A%B1%E8%AE%BE%E8%AE%A1%E7%AC%AC4%E8%8A%82"><span class="toc-number">1.4.</span> <span class="toc-text">第四部分：理论升华：“隐式奖励设计”（第4节）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B1%E5%85%A5%E5%88%86%E6%9E%90pass1-%E5%92%8C-passk-%E7%9A%84-eta-%E6%9B%B2%E7%BA%BF%E7%AC%AC4.1%E8%8A%82"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.1 深入分析：Pass@1
和 Pass@k 的 η
曲线（第4.1节）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E5%BF%B5%E5%8D%87%E5%8D%8E%E9%9A%90%E5%BC%8F%E5%A5%96%E5%8A%B1%E8%AE%BE%E8%AE%A1%E7%AC%AC4.2%E8%8A%82"><span class="toc-number">1.4.2.</span> <span class="toc-text">4.2
概念升华：“隐式奖励设计”（第4.2节）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B%E7%AC%AC5%E8%8A%82%E4%B8%8E%E7%AC%AC6%E8%8A%82"><span class="toc-number">1.5.</span> <span class="toc-text">第五部分：总结与展望（第5节与第6节）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AC%AC5%E8%8A%82"><span class="toc-number">1.5.1.</span> <span class="toc-text">5.1 相关工作（第5节）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA%E7%AC%AC6%E8%8A%82"><span class="toc-number">1.5.2.</span> <span class="toc-text">5.2 结论（第6节）</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/14/DARLING/" title="《共同加强语言模型生成的多样性和质量》论文深度研读">《共同加强语言模型生成的多样性和质量》论文深度研读</a><time datetime="2025-11-14T07:28:02.000Z" title="发表于 2025-11-14 15:28:02">2025-11-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/14/%E7%94%A8%E4%BA%8E%E8%87%AA%E9%80%82%E5%BA%94%E5%B9%B3%E8%A1%A1%E5%A4%A7%E5%9E%8B%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%A9%E7%94%A8%E7%9A%84%20Pass@k%20%E8%AE%AD%E7%BB%83/" title="《Pass@k 训练》论文深度研读">《Pass@k 训练》论文深度研读</a><time datetime="2025-11-14T07:28:02.000Z" title="发表于 2025-11-14 15:28:02">2025-11-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/11/SQL/" title="SQL">SQL</a><time datetime="2025-11-11T09:22:03.000Z" title="发表于 2025-11-11 17:22:03">2025-11-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/31/%E8%BF%9B%E7%A8%8B%E5%90%8C%E6%AD%A5/" title="进程同步">进程同步</a><time datetime="2025-10-31T07:28:02.000Z" title="发表于 2025-10-31 15:28:02">2025-10-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/30/%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B/" title="进程与线程">进程与线程</a><time datetime="2025-10-29T16:00:00.000Z" title="发表于 2025-10-30 00:00:00">2025-10-30</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By kjore</span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.1"></script><script src="/js/main.js?v=5.5.1"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        loader: {
          load: [
            // Four font extension packages (optional)
            //- '[tex]/bbm',
            //- '[tex]/bboldx',
            //- '[tex]/dsfont',
            '[tex]/mhchem'
          ],
          paths: {
            'mathjax-newcm': '[mathjax]/../@mathjax/mathjax-newcm-font',

            //- // Four font extension packages (optional)
            //- 'mathjax-bbm-extension': '[mathjax]/../@mathjax/mathjax-bbm-font-extension',
            //- 'mathjax-bboldx-extension': '[mathjax]/../@mathjax/mathjax-bboldx-font-extension',
            //- 'mathjax-dsfont-extension': '[mathjax]/../@mathjax/mathjax-dsfont-font-extension',
            'mathjax-mhchem-extension': '[mathjax]/../@mathjax/mathjax-mhchem-font-extension'
          }
        },
        output: {
          font: 'mathjax-newcm',
        },
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
          packages: {
            '[+]': [
              'mhchem'
            ]
          }
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          menuOptions: {
            settings: {
              enrich: false  // Turn off Braille and voice narration text automatic generation
            }
          },
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@4.0.0/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>