<!DOCTYPE html><html lang="zh-CN" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>《共同加强语言模型生成的多样性和质量》论文深度研读 | kjore's blog</title><meta name="author" content="kjore"><meta name="copyright" content="kjore"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="论文链接：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2509.02534 深度解析：DARLING论文《联合增强语言模型生成的“多样性”与“质量”》 导言：现代大语言模型的核心困境 大型语言模型（LLMs）的后训练（Post-training）过程，例如基于人类反馈的强化学习（RLHF），显著提升了模型的准确性和实用性。然而，这种优化带来了一个严重的副作用：模型的多样性（diversity">
<meta property="og:type" content="article">
<meta property="og:title" content="《共同加强语言模型生成的多样性和质量》论文深度研读">
<meta property="og:url" content="https://kjore.github.io/2025/11/14/DARLING/index.html">
<meta property="og:site_name" content="kjore&#39;s blog">
<meta property="og:description" content="论文链接：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2509.02534 深度解析：DARLING论文《联合增强语言模型生成的“多样性”与“质量”》 导言：现代大语言模型的核心困境 大型语言模型（LLMs）的后训练（Post-training）过程，例如基于人类反馈的强化学习（RLHF），显著提升了模型的准确性和实用性。然而，这种优化带来了一个严重的副作用：模型的多样性（diversity">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://kjore.github.io/img/head.png">
<meta property="article:published_time" content="2025-11-14T07:28:02.000Z">
<meta property="article:modified_time" content="2025-11-15T02:44:57.495Z">
<meta property="article:author" content="kjore">
<meta property="article:tag" content="论文研读">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://kjore.github.io/img/head.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "《共同加强语言模型生成的多样性和质量》论文深度研读",
  "url": "https://kjore.github.io/2025/11/14/DARLING/",
  "image": "https://kjore.github.io/img/head.png",
  "datePublished": "2025-11-14T07:28:02.000Z",
  "dateModified": "2025-11-15T02:44:57.495Z",
  "author": [
    {
      "@type": "Person",
      "name": "kjore",
      "url": "https://kjore.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://kjore.github.io/2025/11/14/DARLING/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '《共同加强语言模型生成的多样性和质量》论文深度研读',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 8.0.0"></head><body><div id="web_bg" style="background-image: url(/img/bg.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/head.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">18</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-th-large"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/bg.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/head.png" alt="Logo"><span class="site-name">kjore's blog</span></a><a class="nav-page-title" href="/"><span class="site-name">《共同加强语言模型生成的多样性和质量》论文深度研读</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-th-large"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">《共同加强语言模型生成的多样性和质量》论文深度研读</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-11-14T07:28:02.000Z" title="发表于 2025-11-14 15:28:02">2025-11-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-11-15T02:44:57.495Z" title="更新于 2025-11-15 10:44:57">2025-11-15</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>论文链接：<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.02534">https://arxiv.org/abs/2509.02534</a></p>
<h1
id="深度解析darling论文联合增强语言模型生成的多样性与质量">深度解析：DARLING论文《联合增强语言模型生成的“多样性”与“质量”》</h1>
<h2
id="导言现代大语言模型的核心困境">导言：现代大语言模型的核心困境</h2>
<p>大型语言模型（LLMs）的后训练（Post-training）过程，例如基于人类反馈的强化学习（RLHF），显著提升了模型的准确性和实用性。然而，这种优化带来了一个严重的副作用：模型的多样性（diversity）大幅下降
。</p>
<p>在抽象（Abstract）和引言（Introduction）部分（第1节），论文指出了当前方法的核心矛盾：为了追求质量，模型被训练得“过度锐化”（overly
sharpened），其输出分布变得极窄
。这意味着，模型倾向于为同一个提示（prompt）生成高度相似、甚至近乎重复的答案。这种“多样性崩塌”（diversity
collapse）现象，极大地限制了 LLMs
在需要创意和探索性任务（如头脑风暴、讲故事或解决复杂问题）中的应用价值
。</p>
<p>为解决这一挑战，研究人员提出了 <strong>DARLING (Diversity-Aware
Reinforcement Learning)</strong>，即“多样性感知强化学习”框架 。</p>
<p>DARLING
的核心思想是同时优化“质量”和“多样性”两个目标。其机制分为两部分：</p>
<ol type="1">
<li><strong>测量多样性</strong>：DARLING
引入了一个<strong>学习到的划分函数（learned partition
function）</strong>。这本质上是一个语义分类器，用于测量超越表层词汇（lexical）差异的<strong>语义多样性（semantic
diversity）</strong> 。</li>
<li><strong>融合与优化</strong>：在在线强化学习（online
RL）过程中，这个多样性信号与质量奖励（quality
reward）相结合（通过相乘），共同指导模型的梯度更新，鼓励模型生成“有用且不同”（usefully
different）的输出 。</li>
</ol>
<p>引言中提出了本文最引人注目的核心论点：<strong>明确地优化多样性，反过来促进了在线强化学习中的“探索”（exploration），并最终体现为更高质量的响应</strong>
。这一发现挑战了质量与多样性纯粹是对立（trade-off）关系的传统认知。</p>
<h2
id="基础知识标准llm强化学习调优第2节">基础知识：标准LLM强化学习调优（第2节）</h2>
<p>为了理解 DARLING
做了哪些“改变”，我们必须首先理解它所基于的“标准”方法。第2节“符号和准备知识”（Notations
and Preliminaries） 奠定了数学基础。</p>
<h3 id="关键符号">关键符号</h3>
<ul>
<li><span
class="math inline">𝒮</span>：表示所有可能的自然语言序列（token
sequences）的集合。</li>
<li><span
class="math inline"><em>x</em> ∈ 𝒮</span>：表示输入的提示（prompt）。</li>
<li><span
class="math inline"><em>y</em> ∈ 𝒮</span>：表示模型生成的响应（response）。</li>
<li><span
class="math inline"><em>π</em>(⋅|<em>x</em>)</span>：表示语言模型的策略（policy），即给定输入
<span class="math inline"><em>x</em></span>，模型在所有可能的 <span
class="math inline"><em>y</em></span> 上输出的概率分布。</li>
<li><span
class="math inline"><em>r</em>(<em>x</em>, <em>y</em>) → ℝ</span>：表示奖励函数（reward
function），它为一对 <span
class="math inline">(<em>x</em>, <em>y</em>)</span>
打一个标量分数，用于衡量响应的“质量”。</li>
</ul>
<h3 id="公式-1kl约束的优化问题">公式 1：KL约束的优化问题</h3>
<p>LLM 后训练的标准目标是解决一个<strong>KL约束的优化问题（KL
constrained optimization problem）</strong> ：</p>
<p><span
class="math display">max 𝔼<sub><em>x</em> ∼ 𝒟, <em>y</em> ∼ <em>π</em><sub><em>θ</em></sub>(⋅|<em>x</em>)</sub>[<em>r</em>(<em>x</em>, <em>y</em>)] − <em>β</em>𝔻<sub><em>K</em><em>L</em></sub>(<em>π</em><sub><em>θ</em></sub>||<em>π</em><sub><em>r</em><em>e</em><em>f</em></sub>)</span></p>
<p>这个公式的含义可以拆解为：</p>
<ul>
<li><strong>最大化（max）</strong>：我们的目标是最大化整个表达式的值。</li>
<li><strong>质量项（<span
class="math inline">𝔼[<em>r</em>(<em>x</em>, <em>y</em>)]</span>）</strong>：<span
class="math inline">𝔼</span> 代表“期望值”。这一项的意思是，我们希望模型
<span class="math inline"><em>π</em><sub><em>θ</em></sub></span>（<span
class="math inline"><em>θ</em></span> 是模型参数）生成的响应 <span
class="math inline"><em>y</em></span> 在奖励函数 <span
class="math inline"><em>r</em></span>
下的平均得分尽可能高。这是驱动模型提升“质量”的动力。</li>
<li><strong>约束项（<span
class="math inline">−<em>β</em>𝔻<sub><em>K</em><em>L</em></sub>(<em>π</em><sub><em>θ</em></sub>||<em>π</em><sub><em>r</em><em>e</em><em>f</em></sub>)</span>）</strong>：
<ul>
<li><span
class="math inline"><em>π</em><sub><em>r</em><em>e</em><em>f</em></sub></span>
是<strong>参考模型（reference model）</strong>，通常是 RL
训练开始前的模型（例如 SFT 模型）。</li>
<li><span
class="math inline">𝔻<sub><em>K</em><em>L</em></sub>(<em>π</em><sub><em>θ</em></sub>||<em>π</em><sub><em>r</em><em>e</em><em>f</em></sub>)</span>
是<strong>KL散度（Kullback-Leibler divergence）</strong>，它衡量 <span
class="math inline"><em>π</em><sub><em>θ</em></sub></span> 和 <span
class="math inline"><em>π</em><sub><em>r</em><em>e</em><em>f</em></sub></span>
两个概率分布之间的“距离”。</li>
<li><span class="math inline"><em>β</em></span>
是一个超参数，它控制着惩罚的力度，像一根“缰绳”。</li>
</ul></li>
<li><strong>逻辑关系</strong>：这一项是“惩罚项”。它要求 <span
class="math inline"><em>π</em><sub><em>θ</em></sub></span>
不要“漂移”得离 <span
class="math inline"><em>π</em><sub><em>r</em><em>e</em><em>f</em></sub></span>
太远。如果没有这个约束，模型为了最大化 <span
class="math inline"><em>r</em>(<em>x</em>, <em>y</em>)</span>
可能会学会“钻空子”（reward
hacking），生成一些高奖励但无意义的乱码。KL约束保证了模型在学习更高质量的同时，仍然保持着
<span
class="math inline"><em>π</em><sub><em>r</em><em>e</em><em>f</em></sub></span>
所具有的语言连贯性。</li>
</ul>
<h3 id="公式-2-3基线算法-grpo-及其优势函数">公式 2 &amp; 3：基线算法
GRPO 及其“优势函数”</h3>
<p>公式 1 是“目标”，而 <strong>GRPO (Group Relative Policy
Optimization)</strong>
是实现该目标的常用算法之一，也是本文的基线（baseline）。</p>
<p>GRPO 的目标函数（公式 2）非常复杂，但其核心在于 <span
class="math inline"><em>I</em><em>S</em><sub><em>i</em>, <em>t</em></sub> ⋅ <em>A</em><sub><em>i</em>, <em>t</em></sub></span>
这一项。其中 <span
class="math inline"><em>I</em><em>S</em><sub><em>i</em>, <em>t</em></sub></span>
是重要性采样（Importance Sampling）率，而 <span
class="math inline"><em>A</em><sub><em>i</em>, <em>t</em></sub></span>
是<strong>优势函数（Advantage function）</strong>，这是 RL
中的关键信号。</p>
<p><strong>公式 3：GRPO 优势函数</strong></p>
<p><span class="math display">$$
A_{i,t}=\frac{r(x,y_{i})-mean_{j=1}^{n}(r(x,y_{j}))}{std_{j=1}^{n}(r(x,y_{j}))}
$$</span></p>
<ul>
<li><strong>机制</strong>：对于一个提示 <span
class="math inline"><em>x</em></span>，模型生成 <span
class="math inline"><em>n</em></span> 个响应 <span
class="math inline">{<em>y</em><sub>1</sub>, ..., <em>y</em><sub><em>n</em></sub>}</span>。</li>
<li><strong>分子（$r(x,y_{i})-mean(…)
$）</strong>：这是核心信号。它计算的是响应 <span
class="math inline"><em>y</em><sub><em>i</em></sub></span> 的奖励与
<span class="math inline"><em>n</em></span>
个响应的<strong>平均奖励</strong>之间的差值。
<ul>
<li>如果 <span
class="math inline"><em>A</em><sub><em>i</em>, <em>t</em></sub></span>
为正，说明 <span
class="math inline"><em>y</em><sub><em>i</em></sub></span>
“优于平均水平”，算法将增加生成它的概率。</li>
<li>如果 <span
class="math inline"><em>A</em><sub><em>i</em>, <em>t</em></sub></span>
为负，说明 <span
class="math inline"><em>y</em><sub><em>i</em></sub></span>
“差于平均水平”，算法将降低生成它的概率。</li>
</ul></li>
<li><strong>分母（$std(…) $）</strong>：使用 <span
class="math inline"><em>n</em></span>
个响应奖励的<strong>标准差（standard deviation）</strong>
来进行归一化。</li>
<li><strong>一个隐藏的问题（关键）</strong>：使用标准差 <span
class="math inline"><em>s</em><em>t</em><em>d</em></span>
进行归一化看似合理，但论文在第4页和第10页指出，当奖励信号密集且嘈杂时（dense
rewards），这种归一化会“放大噪声”（amplifies the
noise）。如果所有响应的奖励都非常接近（例如 [0.90, 0.91, 0.89]），<span
class="math inline"><em>s</em><em>t</em><em>d</em></span>
将非常小，导致微不足道的奖励差异（0.01）被放大成巨大的训练信号，造成不稳定。DARLING
将会移除这一项。</li>
</ul>
<h2 id="darling-方法详解第3节">DARLING 方法详解（第3节）</h2>
<p>DARLING 如何改进 GRPO 来感知多样性？第3节“Method: DARLING”
详细阐述了其两步机制。</p>
<h3 id="步骤一测量语义多样性">3.1 步骤一：测量“语义”多样性</h3>
<p>首先，我们必须定义什么是“多样性”。</p>
<ul>
<li><strong>问题</strong>：简单的“词汇多样性”（lexical
diversity），如计算不同的
N-grams，很容易被“欺骗”。例如，“狗在跑”和“那只犬科动物在奔跑”在词汇上完全不同，但在“语义”上是等价的。我们希望奖励的是真正新颖的“想法”，而不是同义词替换。</li>
<li><strong>解决方案</strong>：训练一个二元分类器 <span
class="math inline"><em>c</em><em>l</em><em>a</em><em>s</em><em>s</em><em>i</em><em>f</em><em>y</em>(<em>y</em><sub><em>i</em></sub>, <em>y</em><sub><em>j</em></sub>)</span>，如果
<span class="math inline"><em>y</em><sub><em>i</em></sub></span> 和
<span class="math inline"><em>y</em><sub><em>j</em></sub></span>
语义等价，则输出 1，否则输出 0 。</li>
</ul>
<p><strong>公式 4：多样性得分</strong></p>
<p><span class="math display">$$
Div_{d}(y_{i}|y_{1},\cdot\cdot\cdot,y_{n})=\frac{1}{n-1}\sum_{j\ne
i}^{n}d(y_{i},y_{j}).
$$</span></p>
<ul>
<li><strong>释义</strong>：这里的 <span
class="math inline"><em>d</em>(<em>y</em><sub><em>i</em></sub>, <em>y</em><sub><em>j</em></sub>)</span>
是一个“距离”度量，如果 <span
class="math inline"><em>y</em><sub><em>i</em></sub></span> 和 <span
class="math inline"><em>y</em><sub><em>j</em></sub></span>
<strong>语义不同</strong>（即 <span
class="math inline"><em>c</em><em>l</em><em>a</em><em>s</em><em>s</em><em>i</em><em>f</em><em>y</em> = 0</span>），则
<span class="math inline"><em>d</em> = 1</span>；如果语义相同，则 <span
class="math inline"><em>d</em> = 0</span>。</li>
<li><strong>翻译</strong>：这个公式计算的是：“对于响应 <span
class="math inline"><em>y</em><sub><em>i</em></sub></span>，在所有 <span
class="math inline"><em>n</em> − 1</span> 个其他响应中，有多少个与它
<strong>语义不同</strong>？”。<span
class="math inline"><em>D</em><em>i</em><em>v</em><sub><em>d</em></sub></span>
是一个 0 到 1 之间的分数，代表 <span
class="math inline"><em>y</em><sub><em>i</em></sub></span>
在这组响应中的“独特性”或“新颖性”。</li>
</ul>
<p><strong>图 2：笑话示例（Diversity Calculation）</strong> 图 2
完美地演示了这一过程：</p>
<ul>
<li><strong>提示</strong>：“写一个关于编程的短笑话”。生成了 <span
class="math inline"><em>n</em> = 4</span> 个响应。</li>
<li><strong>划分（Partitioning）</strong>：分类器发现，左侧的两个笑话（蓝色）都是关于“bug”的多重含义，因此它们“语义等价”。右侧的两个笑话（紫色、黄色）各自是独特的。</li>
<li><strong>计算 <span
class="math inline"><em>D</em><em>i</em><em>v</em><sub><em>d</em></sub></span></strong>：（<span
class="math inline"><em>n</em> = 4</span>，所以 <span
class="math inline"><em>n</em> − 1 = 3</span>）
<ul>
<li><strong>对于一个蓝色笑话</strong>：它与另一个蓝色笑话语义<strong>相同</strong>（<span
class="math inline"><em>d</em> = 0</span>），与紫色<strong>不同</strong>（<span
class="math inline"><em>d</em> = 1</span>），与黄色<strong>不同</strong>（<span
class="math inline"><em>d</em> = 1</span>）。总和 = 0 + 1 + 1 = 2。</li>
<li><span
class="math inline"><em>D</em><em>i</em><em>v</em><sub><em>d</em></sub></span>
得分 = <span class="math inline">2/3</span>。</li>
<li><strong>对于黄色笑话</strong>：它与两个蓝色笑话和紫色笑话都<strong>不同</strong>（<span
class="math inline"><em>d</em> = 1, <em>d</em> = 1, <em>d</em> = 1</span>）。总和
= 3。</li>
<li><span
class="math inline"><em>D</em><em>i</em><em>v</em><sub><em>d</em></sub></span>
得分 = <span class="math inline">3/3 = 1</span>。</li>
</ul></li>
<li><strong>结论</strong>：黄色笑话（多样性得分1）比蓝色笑话（多样性得分2/3）在多样性上更有价值。</li>
</ul>
<h3 id="步骤二融合奖励与优化">3.2 步骤二：融合奖励与优化</h3>
<p>现在，对于每个响应 <span
class="math inline"><em>y</em><sub><em>i</em></sub></span>，我们有了两个分数：</p>
<ol type="1">
<li><span
class="math inline"><em>r</em>(<em>x</em>, <em>y</em><sub><em>i</em></sub>)</span>：质量（来自奖励模型）</li>
<li><span
class="math inline"><em>D</em><em>i</em><em>v</em><sub><em>d</em></sub>(<em>y</em><sub><em>i</em></sub>)</span>：多样性（来自公式
4）</li>
</ol>
<p><strong>公式 5：DARLING 奖励（Diversity Aware Reward）</strong></p>
<p><span
class="math display"><em>r</em><sub><em>d</em><em>a</em><em>r</em><em>l</em><em>i</em><em>n</em><em>g</em></sub>(<em>x</em>, <em>y</em><sub><em>i</em></sub>|<em>y</em><sub>1</sub>, ..., <em>y</em><sub><em>n</em></sub>) := <em>r</em>(<em>x</em>, <em>y</em><sub><em>i</em></sub>) ⋅ <em>N</em><em>o</em><em>r</em><em>m</em>(<em>D</em><em>i</em><em>v</em><sub><em>d</em></sub>(<em>y</em><sub><em>i</em></sub>|<em>y</em><sub>1</sub>, ..., <em>y</em><sub><em>n</em></sub>))</span></p>
<ul>
<li><strong>机制</strong>：DARLING
的核心创新在于<strong>相乘（multiplies）</strong>。它将质量得分 <span
class="math inline"><em>r</em></span> 与标准化的多样性得分 <span
class="math inline"><em>N</em><em>o</em><em>r</em><em>m</em>(<em>D</em><em>i</em><em>v</em><sub><em>d</em></sub>)</span>（确保在
0-1 之间）相乘。</li>
<li><strong>为什么是乘法而不是加法？</strong>
论文提到加法存在“尺度（scales）”问题
。更深层的解释是，乘法在逻辑上充当了一个
<strong>“与”（AND）门</strong>。
<ul>
<li>要获得高 <span
class="math inline"><em>r</em><sub><em>d</em><em>a</em><em>r</em><em>l</em><em>i</em><em>n</em><em>g</em></sub></span>，一个响应必须<strong>同时具备</strong>高
<span
class="math inline"><em>r</em></span>（高质量）<strong>和</strong>高
<span
class="math inline"><em>D</em><em>i</em><em>v</em><sub><em>d</em></sub></span>（高多样性）。</li>
<li>如果使用加法（<span
class="math inline"><em>r</em> + <em>D</em><em>i</em><em>v</em><sub><em>d</em></sub></span>），一个“高质量但重复”的响应（<span
class="math inline"><em>r</em> = 0.9, <em>D</em><em>i</em><em>v</em><sub><em>d</em></sub> = 0.1</span>，总分1.0）和一个“低质量但新颖”的响应（<span
class="math inline"><em>r</em> = 0.1, <em>D</em><em>i</em><em>v</em><sub><em>d</em></sub> = 0.9</span>，总分1.0）将获得相同的奖励，这显然是错误的。</li>
<li>使用乘法（<span
class="math inline">0.9 × 0.1 = 0.09</span>）与（<span
class="math inline">0.1 × 0.9 = 0.09</span>）相比，真正“高质量且新颖”的响应（<span
class="math inline"><em>r</em> = 0.9, <em>D</em><em>i</em><em>v</em><sub><em>d</em></sub> = 0.9</span>）将获得
<span class="math inline">0.81</span> 的高分，从而被正确地放大。</li>
</ul></li>
</ul>
<p><strong>公式 6：DARLING 最终目标函数</strong> DARLING
的最终目标函数（公式 6）对 GRPO 进行了两项关键修改：</p>
<ol type="1">
<li><strong>使用新奖励</strong>：它使用 <span
class="math inline"><em>r</em><sub><em>d</em><em>a</em><em>r</em><em>l</em><em>i</em><em>n</em><em>g</em></sub></span>
来计算优势函数 <span
class="math inline"><em>A</em><sub><em>i</em>, <em>t</em></sub></span>。</li>
<li><strong>移除 <span
class="math inline"><em>s</em><em>t</em><em>d</em></span>
归一化</strong>：如前所述，它移除了分母中的 <span
class="math inline"><em>s</em><em>t</em><em>d</em></span>
项，以避免放大噪声。</li>
</ol>
<p>因此，DARLING 的优势函数变为：</p>
<p><span
class="math display"><em>A</em><sub><em>i</em>, <em>t</em></sub> = <em>r</em><sub><em>d</em><em>a</em><em>r</em><em>l</em><em>i</em><em>n</em><em>g</em></sub>(<em>x</em>, <em>y</em><sub><em>i</em></sub>|...) − <em>m</em><em>e</em><em>a</em><em>n</em><sub><em>j</em> = 1</sub><sup><em>n</em></sup>(<em>r</em><sub><em>d</em><em>a</em><em>r</em><em>l</em><em>i</em><em>n</em><em>g</em></sub>(<em>x</em>, <em>y</em><sub><em>j</em></sub>|...))</span></p>
<p><strong>图 1：DARLING 流程图</strong> 结合图 1
的流程图，我们可以总结整个过程：</p>
<ol type="1">
<li><strong>生成</strong>：对于一个提示，生成 4 个响应 (a, b, c,
d)。</li>
<li><strong>划分</strong>：语义分类器发现 (a) 和 (b)
语义相同（都关于调试），而 (c) 和 (d) 是独特的。</li>
<li><strong>更新</strong>：
<ul>
<li><strong>标准 GRPO</strong>：假设 (a), (b), (c)
都是高质量的。它们都会获得相同的正面奖励，即使 (a) 和 (b)
是重复的。这导致了多样性崩塌。</li>
<li><strong>DARLING</strong>：(c) 是高质量（高 <span
class="math inline"><em>r</em></span>）<strong>且</strong>高多样性（高
<span
class="math inline"><em>D</em><em>i</em><em>v</em><sub><em>d</em></sub></span>），它获得
<span
class="math inline"><em>r</em> × <em>D</em><em>i</em><em>v</em><sub><em>d</em></sub></span>
的<strong>巨大</strong>正面奖励。(a) 和 (b) 是高质量（高 <span
class="math inline"><em>r</em></span>）<strong>但</strong>低多样性（低
<span
class="math inline"><em>D</em><em>i</em><em>v</em><sub><em>d</em></sub></span>），它们获得
<span
class="math inline"><em>r</em> × <em>D</em><em>i</em><em>v</em><sub><em>d</em></sub></span>
的<strong>微小</strong>正面奖励。</li>
<li><strong>结果</strong>：DARLING
成功地“联合增强”了有用（高质量）且不同（高多样性）的响应。</li>
</ul></li>
</ol>
<h2
id="实验证据一非可验证任务第4节">实验证据（一）：非可验证任务（第4节）</h2>
<p>理论介绍完毕，DARLING 在实践中效果如何？第4节
在“非可验证”（non-verifiable）任务上进行了测试——这些任务没有唯一的正确答案，例如创意写作和指令遵循。</p>
<h3 id="表-1关键定量结果">表 1：关键定量结果</h3>
<p>表 1 提供了核心证据。我们以 Llama-3.1-8B-Instruct
模型为例，分析基线模型、标准 GRPO 和 DARLING 之间的差异：</p>
<table>
<colgroup>
<col style="width: 17%" />
<col style="width: 27%" />
<col style="width: 25%" />
<col style="width: 30%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">模型 (Llama-3.1-8B)</th>
<th style="text-align: center;">质量: AlpacaEval 2.0 (LCWR, %)</th>
<th style="text-align: center;">质量: ArenaHard v2.0 (WR, %)</th>
<th style="text-align: center;">多样性: NoveltyBench (Distinct #)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Llama-3.1-8B (Base)</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">93.9</td>
</tr>
<tr>
<td style="text-align: left;">GRPO (基线)</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">92.8</td>
</tr>
<tr>
<td style="text-align: left;"><strong>DARLING</strong></td>
<td style="text-align: center;"><strong>55.2</strong></td>
<td style="text-align: center;"><strong>68.8</strong></td>
<td style="text-align: center;"><strong>96.0</strong></td>
</tr>
</tbody>
</table>
<p><strong>分析</strong>：</p>
<ol type="1">
<li><strong>GRPO vs. Base (多样性崩塌)</strong>：当使用标准
GRPO（仅优化质量）时，质量指标大幅上升（AE: 31.9 <span
class="math inline">→</span> 48.7），但多样性指标（Distinct
#）<strong>下降</strong>（93.9 <span class="math inline">→</span>
92.8）。这<strong>证实</strong>了引言中提出的“多样性崩塌”问题。</li>
<li><strong>DARLING vs. GRPO (双重胜利)</strong>：与 GRPO 相比，DARLING
不仅在质量上<strong>更高</strong>（AE: 48.7 <span
class="math inline">→</span>
55.2），而且在多样性上也<strong>更高</strong>（Distinct #: 92.8 <span
class="math inline">→</span> 96.0）。</li>
<li><strong>结论</strong>：DARLING
不是一个“权衡”（trade-off），它在两个维度上都优于标准 GRPO。</li>
</ol>
<h3 id="图-3质量-多样性帕累托前沿">图 3：质量-多样性帕累托前沿</h3>
<p>图 3 将表 1 的结果可视化为“帕累托前沿”（Pareto front）图。</p>
<ul>
<li><strong>坐标轴</strong>：X轴是多样性（NoveltyBench
Distinct，越右越好），Y轴是质量（Reward，越高越好）。理想目标是<strong>右上角</strong>。</li>
<li><strong>图线</strong>：GRPO（橙色），DARLING（蓝色）。</li>
<li><strong>分析</strong>：无论是在 8B 还是 70B
模型上，<strong>整条蓝色图线（DARLING）完全位于整条橙色图线（GRPO）的右上方</strong>。</li>
<li><strong>含义</strong>：这被称为“帕累托改进”（Pareto
improvement）。这意味着 DARLING
是一种在所有维度上都更优越的训练方法。对于 GRPO
上的任意一点（代表某种质量-多样性平衡），DARLING
总能提供一个在质量和多样性上“双重更优”的点。</li>
</ul>
<h3 id="节定性分析图-4-5">4.3 节：定性分析（图 4 &amp; 5）</h3>
<p>数据很好，但实际体验如何？</p>
<p><strong>图 4：DARLING 的优势所在</strong></p>
<ul>
<li><strong>内容</strong>：该图分析了 DARLING 在
EQ-Bench（创意写作基准）的具体评分标准上的胜率。</li>
<li><strong>Top 2 胜率</strong>：“有趣和原创”（Interesting and Original,
88.7%），“避免陈词滥调”（Avoids Cliches, 88.3%）。</li>
<li><strong>分析</strong>：这有力地证明了 DARLING
正在优化它声称要优化的东西。因为它明确地奖励“语义独特性”，所以它在衡量“创意”和“原创性”的指标上表现最佳。</li>
</ul>
<p><strong>图 5：钱包示例（最直观的对比）</strong></p>
<ul>
<li><strong>提示</strong>：“我正在网购一款新皮夹…
请只给我一个建议。”（并行生成 4 个响应）</li>
<li><strong>基线 (Llama-3.3-70B)</strong>：
<ol type="1">
<li>Amazon.</li>
<li>Amazon.</li>
<li>Amazon.</li>
<li>Amazon.</li>
</ol></li>
<li><strong>DARLING (Llama-3.3-70B)</strong>：
<ol type="1">
<li><strong>Fossil.</strong> (专业做皮革，质量好…)</li>
<li><strong>Bellroy.</strong> (设计周到，轻薄，有 RFID 阻挡…)</li>
<li><strong>Coach.com.</strong> (高质量图片，详细的皮革描述…)</li>
<li><strong>Bellroy.</strong> (款式丰富，材料信息详细，有保修…)</li>
</ol></li>
<li><strong>分析</strong>：这是一个惊人的对比。基线模型完全陷入了“多样性崩塌”。而
DARLING
提供了四个<strong>不同、高质量、有用且理由充分</strong>的答案。</li>
<li><strong>关键细节</strong>：请注意，DARLING
输出了两次“Bellroy”，但第二次给出了<strong>完全不同的理由</strong>（例如“鞣制方法”）。根据
DARLING
的语义分类器，这（可能）被认为是<strong>语义不同</strong>的，因此获得了多样性奖励。这展示了该方法的智能之处：它奖励的是新颖的“论点”，而不仅仅是新颖的“词汇”。</li>
</ul>
<h2
id="实验证据二可验证任务第5节">实验证据（二）：可验证任务（第5节）</h2>
<p>第5节
提出了一个更激进的问题：多样性对“数学”这类有唯一正确答案的任务有帮助吗？</p>
<h3 id="关键指标pass1-vs.-passk">关键指标：<code>pass@1</code>
vs. <code>pass@k</code></h3>
<p>理解这里的指标至关重要 ：</p>
<ul>
<li><strong><code>pass@1</code>
(质量)</strong>：衡量“解题质量”。模型只生成 1 个答案，它是否正确？</li>
<li><strong><code>pass@k</code>
(多样性/探索)</strong>：衡量“解题多样性”。模型生成 <span
class="math inline"><em>k</em></span> 个答案（例如 <span
class="math inline"><em>k</em> = 128</span>），这 <span
class="math inline"><em>k</em></span> 个答案中是否<strong>至少有 1
个</strong>是正确的？</li>
<li><strong>逻辑关系</strong>：要提高
<code>pass@k</code>，模型<strong>必须</strong>具有多样性。如果模型只会用一种错误的方法，并重复
128 次，那么 <code>pass@k</code> 依然是 0。只有尝试 <span
class="math inline"><em>k</em></span> 种不同的方法，<code>pass@k</code>
才有机会提高。</li>
</ul>
<h3 id="图-6数学任务结果">图 6：数学任务结果</h3>
<p>图 6 显示了在多个数学竞赛基准（如 AIME, HMMT）上的
<code>pass@k</code> 曲线。</p>
<ul>
<li><strong>坐标轴</strong>：X轴是 <span
class="math inline"><em>k</em></span>（从 1 到 128），Y轴是
<code>pass@k</code>（%）。</li>
<li><strong>图线</strong>：DARLING（蓝色），GRPO（橙色）。</li>
<li><strong>分析</strong>：
<ol type="1">
<li><strong>当 <span class="math inline"><em>k</em> = 128</span>
时</strong>（X轴最右端）：蓝线远高于橙线（例如，在 HMMT 2025 / Qwen3-14B
上，DARLING 约 50.41% vs GRPO 约 34.44%）。
<ul>
<li><strong>解读</strong>：这符合预期。DARLING
被训练得更多样化，因此它在 128
次尝试中能探索更多不同的解题路径，从而更有可能“碰巧”找到一条正确的路径。</li>
</ul></li>
<li><strong>当 <span class="math inline"><em>k</em> = 1</span>
时</strong>（X轴最左端）：蓝线<strong>也</strong>高于橙线（例如，在 HMMT
2025 / Qwen3-14B 上，DARLING 约 17.21% vs GRPO 约 10.86%）。
<ul>
<li><strong>解读</strong>：这是<strong>最引人注目</strong>的发现，也是论文核心论点的最终证明。</li>
</ul></li>
</ol></li>
<li><strong>为什么？（探索假说）</strong>
<ul>
<li><strong>GRPO（橙线）</strong>：GRPO
倾向于“利用”（exploit）。它很快找到一条“还不错”的解题路径，然后不断优化这条路径，导致其陷入“局部最优解”（local
optimum）。如果这条路径恰好是错的，GRPO 永远也找不到正确的答案。</li>
<li><strong>DARLING（蓝线）</strong>：DARLING
因为其多样性奖励，在重复使用同一条路径时会受到“惩罚”（因为 <span
class="math inline"><em>D</em><em>i</em><em>v</em><sub><em>d</em></sub></span>
很低）。这迫使它去“探索”（explore）其他完全不同的解题路径。</li>
<li><strong>结果</strong>：通过被迫探索这些新颖的、不同的路径，DARLING
“偶然发现”了 GRPO
会错过的、更优的、最终正确的解题路径。这种探索最终提高了它的“最佳答案”的质量，即
<code>pass@1</code>。</li>
<li><strong>结论</strong>：这证实了引言中的假说——多样性不仅是目标，更是一种<strong>实现探索的机制</strong>，而这种探索最终带来了更高质量的解决方案。</li>
</ul></li>
</ul>
<h2 id="设计合理性消融研究第6节">设计合理性：消融研究（第6节）</h2>
<p>第6节 是一个“消融研究”（Ablations），它通过对比实验来证明 DARLING
的每一个设计决策都是必要的。</p>
<h3 id="表-2乘法darling-vs.-加法">表 2：乘法（DARLING） vs. 加法</h3>
<ul>
<li><strong>实验</strong>：比较
<code>Quality x partition</code>（乘法）和
<code>Quality + partition</code>（加法）。</li>
<li><strong>数据</strong>：
<ul>
<li>加法：AE 质量 = 53.17, NoveltyBench 多样性 = 5.23</li>
<li>乘法 (DARLING)：AE 质量 = 55.15, NoveltyBench 多样性 = 5.49</li>
</ul></li>
<li><strong>结论</strong>：乘法在质量和多样性上<strong>均</strong>优于加法。这证实了我们在
3.2 节中讨论的“与”（AND）门逻辑。</li>
</ul>
<h3 id="表-3-4语义darling-vs.-词汇n-gram">表 3 &amp; 4：语义（DARLING）
vs. 词汇（N-gram）</h3>
<ul>
<li><strong>实验</strong>：如果我们不用昂贵的语义分类器，而是用简单的
<code>4-gram</code> 词汇多样性得分会怎样？</li>
<li><strong>表 3 (创意写作)</strong>：
<ul>
<li><code>4-gram</code>：AE = 53.82, Distinct = 3.59</li>
<li><code>DARLING</code>：AE = 55.15, Distinct = 5.49</li>
<li><strong>结论</strong>：词汇多样性在两个指标上都更差。</li>
</ul></li>
<li><strong>表 4 (数学)</strong>： 这是最关键的证据。
<ul>
<li><strong>数据 (Avg. pass@1 on Qwen3-4B)</strong>：
<ul>
<li>GRPO (基线)：23.40</li>
<li><strong>DARLING (语义)</strong>：<strong>26.91</strong> (+3.51)</li>
<li><strong><code>4-gram</code> (词汇)</strong>：<strong>22.49</strong>
(-0.91)</li>
</ul></li>
</ul></li>
<li><strong>分析（奖励黑客）</strong>：使用词汇多样性（4-gram）的性能<strong>低于</strong>基线！它<strong>损害</strong>了模型。
<ul>
<li><strong>为什么？</strong> 论文在 6.2 节和附录 G
中解释了“奖励黑客”（Reward Hacking）。模型为了让 4-grams
看起来不同，学会在正确答案（例如“最终答案是
8”）之后，添加大量无意义的“反思”或“废话”（例如“…我觉得这道题很难…”）。</li>
<li>这种行为在词汇上是“多样化”的，因此获得了奖励，但这污染了训练信号，使模型在数学上变得更糟。</li>
<li>而 DARLING
的<strong>语义分类器</strong>是免疫的。它知道“答案是8”和“答案是8 +
废话”在语义上是<strong>等价</strong>的，因此会给予 <span
class="math inline"><em>D</em><em>i</em><em>v</em><sub><em>d</em></sub> = 0</span>
的惩罚。这证明了昂贵的语义分类器是<strong>绝对必要</strong>的。</li>
</ul></li>
</ul>
<h3 id="表-5优势函数归一化advantage-normalization">表
5：优势函数归一化（Advantage Normalization）</h3>
<ul>
<li><strong>实验</strong>：我们在公式 3 中看到的 <span
class="math inline"><em>s</em><em>t</em><em>d</em></span>
归一化（标准差）到底有没有用？DARLING（公式 6）移除了它。</li>
<li><strong>数据 (针对 DARLING/partition 方法)</strong>：
<ul>
<li><strong>使用</strong> <span
class="math inline"><em>s</em><em>t</em><em>d</em></span> 归一化：AE
质量 = 51.64, NoveltyBench 多样性 = 3.35</li>
<li><strong>不使用</strong> <span
class="math inline"><em>s</em><em>t</em><em>d</em></span> 归一化
(DARLING)：AE 质量 = 55.15, NoveltyBench 多样性 = 5.49</li>
</ul></li>
<li><strong>结论</strong>：<strong>移除 <span
class="math inline"><em>s</em><em>t</em><em>d</em></span>
归一化带来了巨大的性能提升</strong>。</li>
<li><strong>分析</strong>：这证实了 2.3 节的猜想和 6.3 节的理论
：在奖励信号密集且嘈杂的 LLM
训练中，标准差归一化确实会放大噪声并损害性能。</li>
<li><strong>最终配方</strong>：DARLING 的完整“配方”——（语义分类器 +
乘法奖励 + 无 <span
class="math inline"><em>s</em><em>t</em><em>d</em></span>
归一化的优势函数）——被证明是最佳的组合。</li>
</ul>
<h2 id="结论与相关工作第78节">结论与相关工作（第7、8节）</h2>
<ul>
<li><strong>相关工作（第7节）</strong>：论文将 DARLING
与其他多样性方法进行了区分 。
<ul>
<li>不同于 DivPO 等“离线”（offline）方法，DARLING
是“在线”（online）RL，这使其能够主动“探索”新状态。</li>
<li>不同于在“推理时”（inference-time）调整采样温度，DARLING
在“训练时”（training-time）改变了模型本身，使其基础能力更强。</li>
</ul></li>
<li><strong>总结（第8节）</strong>：
<ol type="1">
<li><strong>问题</strong>：LLM 后训练会扼杀多样性（多样性崩塌）。</li>
<li><strong>假说</strong>：解决多样性问题可以通过“探索”机制反过来提升质量。</li>
<li><strong>方法</strong>：DARLING = (语义分类器 <span
class="math inline">×</span> 质量奖励 <span class="math inline">×</span>
无 <span class="math inline"><em>s</em><em>t</em><em>d</em></span>
归一化优势函数)。</li>
<li><strong>证据 1 (创意任务)</strong>：DARLING
同时提升了质量和多样性（表 1, 图 3-5）。</li>
<li><strong>证据 2 (数学任务)</strong>：DARLING
通过提升多样性（<code>pass@k</code>）来促进探索，从而提升了
<code>pass@1</code> 质量（图 6）。</li>
<li><strong>证据 3 (消融)</strong>：DARLING
的每一个设计决策（语义、乘法、无 <span
class="math inline"><em>s</em><em>t</em><em>d</em></span>）都被证明优于其替代方案（表
2-5）。</li>
</ol></li>
</ul>
<p>综上所述，DARLING 框架不仅成功解决了 LLM
后训练中的多样性崩塌问题，更重要的是，它揭示并利用了“多样性”与“探索”和“质量”之间的深刻联系，为训练更强大、更有创造力的
AI 模型提供了一条经过严谨验证的路径。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://kjore.github.io">kjore</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://kjore.github.io/2025/11/14/DARLING/">https://kjore.github.io/2025/11/14/DARLING/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://kjore.github.io" target="_blank">kjore's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/">论文研读</a></div><div class="post-share"><div class="social-share" data-image="/img/head.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/11/14/%E7%94%A8%E4%BA%8E%E8%87%AA%E9%80%82%E5%BA%94%E5%B9%B3%E8%A1%A1%E5%A4%A7%E5%9E%8B%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%A9%E7%94%A8%E7%9A%84%20Pass@k%20%E8%AE%AD%E7%BB%83/" title="《Pass@k 训练》论文深度研读"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">《Pass@k 训练》论文深度研读</div></div><div class="info-2"><div class="info-item-1">原文链接：http://arxiv.org/abs/2508.10751 《Pass@k 训练》论文深度研读 这篇论文《Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models》，是大型语言模型（LLM）推理能力训练领域的一篇重要技术报告。它不仅提出了一种新颖且效果显著的训练方法，更重要的是，它为解决强化学习（ReinCforcement Learning, RL）中的一个经典难题——“探索与利用的平衡”——提供了深刻的洞见。 我们的目标是，在阅读完本报告后，您不仅能理解这篇论文“做了什么”，更能深刻理解它“为什么这么做”以及“它背后的数学原理”。  第一部分：问题的提出（摘要与第1节：引言） 在进入复杂的公式之前，我们必须首先理解论文试图解决的“问题”是什么。 1.1 核心场景：RLVR 与大型推理模型 论文的背景是使用强化学习与可验证奖励（Reinforcement Learning with Verifiable ...</div></div></div></a><a class="pagination-related" href="/2025/11/11/SQL/" title="SQL"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">SQL</div></div><div class="info-2"><div class="info-item-1">SQL数据定义  char(n) :n个长度的固定字符串 varchar(n) :最大n个长度的可变长字符串 int :整数 smallint :小整数 numeric(p,d) :这个数有p位数字，小数点后d位数字。如numeric(3,1)可以存储33.3 ,44.4 float(n) :精度至少为n的单精度浮点数 double :双精度浮点数 date :日期，格式YYYY-MM-DD time :时间，格式HH:MM:SS timestamp :时间戳，格式YYYY-MM-DD HH:MM:SS  DDL DDL是数据库定义语言，用于创建修改删除数据库的结构 创建数据库 1234567891011create table table_name(    id int,    name char(10) not null,    age int,    gender char(1) default &#x27;M&#x27;,    address varchar(50),    primary key(id),    unique(name...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/11/14/%E7%94%A8%E4%BA%8E%E8%87%AA%E9%80%82%E5%BA%94%E5%B9%B3%E8%A1%A1%E5%A4%A7%E5%9E%8B%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%A9%E7%94%A8%E7%9A%84%20Pass@k%20%E8%AE%AD%E7%BB%83/" title="《Pass@k 训练》论文深度研读"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-14</div><div class="info-item-2">《Pass@k 训练》论文深度研读</div></div><div class="info-2"><div class="info-item-1">原文链接：http://arxiv.org/abs/2508.10751 《Pass@k 训练》论文深度研读 这篇论文《Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models》，是大型语言模型（LLM）推理能力训练领域的一篇重要技术报告。它不仅提出了一种新颖且效果显著的训练方法，更重要的是，它为解决强化学习（ReinCforcement Learning, RL）中的一个经典难题——“探索与利用的平衡”——提供了深刻的洞见。 我们的目标是，在阅读完本报告后，您不仅能理解这篇论文“做了什么”，更能深刻理解它“为什么这么做”以及“它背后的数学原理”。  第一部分：问题的提出（摘要与第1节：引言） 在进入复杂的公式之前，我们必须首先理解论文试图解决的“问题”是什么。 1.1 核心场景：RLVR 与大型推理模型 论文的背景是使用强化学习与可验证奖励（Reinforcement Learning with Verifiable ...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/head.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">kjore</div><div class="author-info-description">什么都不懂</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">18</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/kjore"><i class="fab fa-github"></i><span>关注我</span></a><div class="card-info-social-icons"><a class="social-icon" href="mailto:zhangjingye@bupt.edu.cn" target="_blank" title="邮箱"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90darling%E8%AE%BA%E6%96%87%E8%81%94%E5%90%88%E5%A2%9E%E5%BC%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%94%9F%E6%88%90%E7%9A%84%E5%A4%9A%E6%A0%B7%E6%80%A7%E4%B8%8E%E8%B4%A8%E9%87%8F"><span class="toc-number">1.</span> <span class="toc-text">深度解析：DARLING论文《联合增强语言模型生成的“多样性”与“质量”》</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%BC%E8%A8%80%E7%8E%B0%E4%BB%A3%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A0%B8%E5%BF%83%E5%9B%B0%E5%A2%83"><span class="toc-number">1.1.</span> <span class="toc-text">导言：现代大语言模型的核心困境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%A0%87%E5%87%86llm%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%B0%83%E4%BC%98%E7%AC%AC2%E8%8A%82"><span class="toc-number">1.2.</span> <span class="toc-text">基础知识：标准LLM强化学习调优（第2节）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E9%94%AE%E7%AC%A6%E5%8F%B7"><span class="toc-number">1.2.1.</span> <span class="toc-text">关键符号</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AC%E5%BC%8F-1kl%E7%BA%A6%E6%9D%9F%E7%9A%84%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98"><span class="toc-number">1.2.2.</span> <span class="toc-text">公式 1：KL约束的优化问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AC%E5%BC%8F-2-3%E5%9F%BA%E7%BA%BF%E7%AE%97%E6%B3%95-grpo-%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.3.</span> <span class="toc-text">公式 2 &amp; 3：基线算法
GRPO 及其“优势函数”</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#darling-%E6%96%B9%E6%B3%95%E8%AF%A6%E8%A7%A3%E7%AC%AC3%E8%8A%82"><span class="toc-number">1.3.</span> <span class="toc-text">DARLING 方法详解（第3节）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4%E4%B8%80%E6%B5%8B%E9%87%8F%E8%AF%AD%E4%B9%89%E5%A4%9A%E6%A0%B7%E6%80%A7"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 步骤一：测量“语义”多样性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4%E4%BA%8C%E8%9E%8D%E5%90%88%E5%A5%96%E5%8A%B1%E4%B8%8E%E4%BC%98%E5%8C%96"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 步骤二：融合奖励与优化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E8%AF%81%E6%8D%AE%E4%B8%80%E9%9D%9E%E5%8F%AF%E9%AA%8C%E8%AF%81%E4%BB%BB%E5%8A%A1%E7%AC%AC4%E8%8A%82"><span class="toc-number">1.4.</span> <span class="toc-text">实验证据（一）：非可验证任务（第4节）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A8-1%E5%85%B3%E9%94%AE%E5%AE%9A%E9%87%8F%E7%BB%93%E6%9E%9C"><span class="toc-number">1.4.1.</span> <span class="toc-text">表 1：关键定量结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE-3%E8%B4%A8%E9%87%8F-%E5%A4%9A%E6%A0%B7%E6%80%A7%E5%B8%95%E7%B4%AF%E6%89%98%E5%89%8D%E6%B2%BF"><span class="toc-number">1.4.2.</span> <span class="toc-text">图 3：质量-多样性帕累托前沿</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8A%82%E5%AE%9A%E6%80%A7%E5%88%86%E6%9E%90%E5%9B%BE-4-5"><span class="toc-number">1.4.3.</span> <span class="toc-text">4.3 节：定性分析（图 4 &amp; 5）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E8%AF%81%E6%8D%AE%E4%BA%8C%E5%8F%AF%E9%AA%8C%E8%AF%81%E4%BB%BB%E5%8A%A1%E7%AC%AC5%E8%8A%82"><span class="toc-number">1.5.</span> <span class="toc-text">实验证据（二）：可验证任务（第5节）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E9%94%AE%E6%8C%87%E6%A0%87pass1-vs.-passk"><span class="toc-number">1.5.1.</span> <span class="toc-text">关键指标：pass@1
vs. pass@k</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE-6%E6%95%B0%E5%AD%A6%E4%BB%BB%E5%8A%A1%E7%BB%93%E6%9E%9C"><span class="toc-number">1.5.2.</span> <span class="toc-text">图 6：数学任务结果</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BE%E8%AE%A1%E5%90%88%E7%90%86%E6%80%A7%E6%B6%88%E8%9E%8D%E7%A0%94%E7%A9%B6%E7%AC%AC6%E8%8A%82"><span class="toc-number">1.6.</span> <span class="toc-text">设计合理性：消融研究（第6节）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A8-2%E4%B9%98%E6%B3%95darling-vs.-%E5%8A%A0%E6%B3%95"><span class="toc-number">1.6.1.</span> <span class="toc-text">表 2：乘法（DARLING） vs. 加法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A8-3-4%E8%AF%AD%E4%B9%89darling-vs.-%E8%AF%8D%E6%B1%87n-gram"><span class="toc-number">1.6.2.</span> <span class="toc-text">表 3 &amp; 4：语义（DARLING）
vs. 词汇（N-gram）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A8-5%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B0%E5%BD%92%E4%B8%80%E5%8C%96advantage-normalization"><span class="toc-number">1.6.3.</span> <span class="toc-text">表
5：优势函数归一化（Advantage Normalization）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AC%AC78%E8%8A%82"><span class="toc-number">1.7.</span> <span class="toc-text">结论与相关工作（第7、8节）</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/12/29/%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%E4%B8%8E%E8%AE%BE%E8%AE%A1/" title="算法分析与设计">算法分析与设计</a><time datetime="2025-12-29T09:22:03.000Z" title="发表于 2025-12-29 17:22:03">2025-12-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/12/22/MySQL%E4%BD%BF%E7%94%A8/" title="MySQL命令行使用">MySQL命令行使用</a><time datetime="2025-12-22T09:22:03.000Z" title="发表于 2025-12-22 17:22:03">2025-12-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/12/20/%E7%A3%81%E7%9B%98/" title="无标题">无标题</a><time datetime="2025-12-20T08:43:42.263Z" title="发表于 2025-12-20 16:43:42">2025-12-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/12/10/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E4%B8%8E%E4%BC%98%E5%8C%96/" title="查询处理与优化">查询处理与优化</a><time datetime="2025-12-10T09:22:03.000Z" title="发表于 2025-12-10 17:22:03">2025-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/12/10/%E7%B4%A2%E5%BC%95/" title="物理存储结构与索引">物理存储结构与索引</a><time datetime="2025-12-10T09:22:03.000Z" title="发表于 2025-12-10 17:22:03">2025-12-10</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By kjore</span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.1"></script><script src="/js/main.js?v=5.5.1"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        loader: {
          load: [
            // Four font extension packages (optional)
            //- '[tex]/bbm',
            //- '[tex]/bboldx',
            //- '[tex]/dsfont',
            '[tex]/mhchem'
          ],
          paths: {
            'mathjax-newcm': '[mathjax]/../@mathjax/mathjax-newcm-font',

            //- // Four font extension packages (optional)
            //- 'mathjax-bbm-extension': '[mathjax]/../@mathjax/mathjax-bbm-font-extension',
            //- 'mathjax-bboldx-extension': '[mathjax]/../@mathjax/mathjax-bboldx-font-extension',
            //- 'mathjax-dsfont-extension': '[mathjax]/../@mathjax/mathjax-dsfont-font-extension',
            'mathjax-mhchem-extension': '[mathjax]/../@mathjax/mathjax-mhchem-font-extension'
          }
        },
        output: {
          font: 'mathjax-newcm',
        },
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
          packages: {
            '[+]': [
              'mhchem'
            ]
          }
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          menuOptions: {
            settings: {
              enrich: false  // Turn off Braille and voice narration text automatic generation
            }
          },
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@4.0.0/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>